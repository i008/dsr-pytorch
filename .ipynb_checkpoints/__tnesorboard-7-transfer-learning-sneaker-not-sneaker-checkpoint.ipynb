{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an image classifier able to find sneakers in instagram posts\n",
    "\n",
    "The data comprises of few thousand images of sneakers collected using google images and instagram\n",
    "and few thousand images of sneakers.    \n",
    "Your goal is to use what you learned from previous examples and create a sneaker-not-sneaker binary classifier.\n",
    "\n",
    "The task comprises of multiple sub-tasks that you need to do to build the classifier.\n",
    "\n",
    "1. Create a dataset able to load data from new_meta_sneakers.csv\n",
    "2. Create a fine tune binary classification architecture.\n",
    "3. Create a training loop and train your model.\n",
    "\n",
    "![title](static/sneakers.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the bottom of the following cell you see the data you will work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torch import nn\n",
    "import easyimages\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pretrainedmodels.models import resnet50\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import pathlib\n",
    "from imgaug import augmenters as iaa\n",
    "from torch.utils.tensorboard import writer\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tags</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1782</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/i008/small_sneaker_not_sneaker/nontsneak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1237</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/i008/small_sneaker_not_sneaker/nontsneak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1352</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/i008/small_sneaker_not_sneaker/nontsneak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1928</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/i008/small_sneaker_not_sneaker/nontsneak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1463</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/i008/small_sneaker_not_sneaker/nontsneak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  tags                                         image_path\n",
       "0   1782     0  /home/i008/small_sneaker_not_sneaker/nontsneak...\n",
       "1   1237     0  /home/i008/small_sneaker_not_sneaker/nontsneak...\n",
       "2   1352     0  /home/i008/small_sneaker_not_sneaker/nontsneak...\n",
       "3   1928     0  /home/i008/small_sneaker_not_sneaker/nontsneak...\n",
       "4   1463     0  /home/i008/small_sneaker_not_sneaker/nontsneak..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_SNEAKER_NOT_SNEAKER_DATASET = '/home/i008/small_sneaker_not_sneaker'\n",
    "base_path = pathlib.Path(PATH_TO_SNEAKER_NOT_SNEAKER_DATASET)\n",
    "\n",
    "classes = []\n",
    "paths = []\n",
    "for p in base_path.glob('*/*'):\n",
    "    relative_path = p.relative_to(base_path)\n",
    "    classes.append(str(relative_path.parent))\n",
    "    paths.append(p)\n",
    "    \n",
    "df = pd.DataFrame({\"tags\": classes, \"image_path\": paths}).sample(frac=1).reset_index()\n",
    "df.tags = df.tags.map({'sneaker': 1, 'nontsneaker': 0})\n",
    "df.head()\n",
    "\n",
    "# df[df.image_path.apply(str).str.contains('1238')].image_path.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the torch Dataset.\n",
    "\n",
    "First thing we need to do is create a dataset able to load our data. Since our metadata is stored in a csv file, our \n",
    "dataset should accept this file as a base source of what needs to be loaded.\n",
    "\n",
    "Our dataset should also support augumentations and a \"inference\" mode wich disables them for predicting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class OneClassImageClassificationDataset(Dataset):\n",
    "    def __init__(self, annotations, image_transform, augmenter=None):\n",
    "        \"\"\"\n",
    "        annotations is a pandas dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.annotations = annotations\n",
    "        self.image_transform = image_transform\n",
    "        self.augmenter = augmenter\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the annotations dataframe\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Using methods you wrote:\n",
    "        1 - load image from disk for given index  (self.load_from_disk)\n",
    "        2 - transform image (self.image_transform)\n",
    "        3 - Load target (self.load_target)\n",
    "        return Xi, yi\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        Xi = self.load_from_disk(index)\n",
    "        \n",
    "        if self.augmenter is not None:\n",
    "            Xi = self.augment(self.augmenter, Xi)\n",
    "                  \n",
    "        Xi = self.image_transform(Xi)\n",
    "        yi = self.load_target(index)\n",
    "        return Xi, yi\n",
    "\n",
    "    def load_to_pil(self, uri):\n",
    "        \"\"\"\n",
    "        Write a helper function that uses PIL.Image to load a file and returns it\n",
    "        \"\"\"\n",
    "\n",
    "        image_pil = Image.open(uri)\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "        # image_pil = YOUR CODE HERE\n",
    "        return image_pil\n",
    "\n",
    "\n",
    "    def load_from_disk(self, index):\n",
    "        \"\"\"\n",
    "        Loads an image from disk given a index.\n",
    "        It gets the path of an image with the corresponding index from the metadata \n",
    "        It passes the URI to the self.load_to_pil and returns a PIL.Image\n",
    "        \"\"\"\n",
    "        image_path = self.annotations.iloc[index]['image_path']\n",
    "        #image_path = # YOUR CODE HERE\n",
    "        return self.load_to_pil(image_path)\n",
    "\n",
    "    def load_target(self, index):\n",
    "        \"\"\"\n",
    "        This function should get the tag for a given index from the annotations dataframe\n",
    "        You .iloc can become useful.    \n",
    "        This methods should return, either a 0 or a 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        #label = # YOUR CODE HERE\n",
    "        label = self.annotations.iloc[index]['tags']\n",
    "\n",
    "        return label\n",
    "    \n",
    "    def augment(self, augmenter, image):\n",
    "        augmenter = augmenter.to_deterministic()\n",
    "        img_aug = augmenter.augment_image(np.array(image))\n",
    "        img_aug = Image.fromarray(img_aug)\n",
    "        return img_aug\n",
    "\n",
    "    \n",
    "class BaseSampler(Sampler):\n",
    "    def __init__(self, df, n_samples):\n",
    "        self.df = df\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self._get_sample())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def _get_sample(self):\n",
    "        return np.random.choice(len(self.df), self.n_samples, replace=False)\n",
    "        \n",
    "\n",
    "def binary_classification_model():\n",
    "    \"\"\"\n",
    "    Write a function that loads a resnet50 model from pretrainedmodels, freezes its layers\n",
    "    replaces the last_linear with the proper output number. As we did in previous example.\n",
    "    replace avgpool with adaptiv pooling.\n",
    "    \"\"\"\n",
    "    model = resnet50()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    inft = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_features=inft, out_features=2)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    \n",
    "    # model = YOUR CODE HERE\n",
    "    return model\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "aug_seq = iaa.Sequential([\n",
    "    iaa.Fliplr(p=0.5),\n",
    "    iaa.Sometimes(\n",
    "        0.3,\n",
    "        iaa.Multiply((0.9, 1.2))\n",
    "    ),\n",
    "    iaa.Sometimes(\n",
    "        0.3,\n",
    "        iaa.AdditiveGaussianNoise()\n",
    "    ),\n",
    "    iaa.Affine(\n",
    "        scale=(0.5, 2),\n",
    "        translate_percent=(-0.2, 0.2)\n",
    "    )\n",
    "])\n",
    "def augment(self, augmenter, image):\n",
    "    augmenter = augmenter.to_deterministic()\n",
    "    img_aug = augmenter.augment_image(np.array(image))\n",
    "    img_aug = Image.fromarray(img_aug)\n",
    "    return img_aug\n",
    "\n",
    "\n",
    "\n",
    "def binary_classification_model():\n",
    "    \"\"\"\n",
    "    Write a function that loads a resnet50 model from pretrainedmodels, freezes its layers\n",
    "    replaces the last_linear with the proper output number. As we did in previous example.\n",
    "    replace avgpool with adaptiv pooling.\n",
    "    \"\"\"\n",
    "    model = resnet50()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    inft = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_features=inft, out_features=2)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    \n",
    "    # model = YOUR CODE HERE\n",
    "    return model\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# SPLIT the dataframe into df_train, df_test (thing about using sklearn.model_selection.train_test_split)\n",
    "df_train, df_test = train_test_split(df, train_size=0.9)\n",
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "image_transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "# YOUR CODE define image_transform_test\n",
    "image_transform_test = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "# YOUR CODE define the crieterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net = binary_classification_model()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# YOUR CODE\n",
    "# Instantiate the OneClassImageClassificationDatasets\n",
    "train_ds = OneClassImageClassificationDataset(df_train, image_transform=image_transform_train, augmenter=aug_seq)\n",
    "test_ds = OneClassImageClassificationDataset(df_test, image_transform=image_transform_test)\n",
    "\n",
    "# initialize the BaseSampler\n",
    "bs = BaseSampler(train_ds, 1000)\n",
    "\n",
    "#YOUR CODE\n",
    "#Initialize your DataLoader (using datasets)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=bs)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_classification_model():\n",
    "    \"\"\"\n",
    "    Write a function that loads a resnet50 model from pretrainedmodels, freezes its layers\n",
    "    replaces the last_linear with the proper output number. As we did in previous example.\n",
    "    replace avgpool with adaptiv pooling.\n",
    "    \"\"\"\n",
    "    model = resnet50()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    inft = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_features=inft, out_features=2)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    \n",
    "    # model = YOUR CODE HERE\n",
    "    return model\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# SPLIT the dataframe into df_train, df_test (thing about using sklearn.model_selection.train_test_split)\n",
    "df_train, df_test = train_test_split(df, train_size=0.9)\n",
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "image_transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "# YOUR CODE define image_transform_test\n",
    "image_transform_test = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "# YOUR CODE define the crieterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net = binary_classification_model()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# YOUR CODE\n",
    "# Instantiate the OneClassImageClassificationDatasets\n",
    "train_ds = OneClassImageClassificationDataset(df_train, image_transform=image_transform_train, augmenter=aug_seq)\n",
    "test_ds = OneClassImageClassificationDataset(df_test, image_transform=image_transform_test)\n",
    "\n",
    "# initialize the BaseSampler\n",
    "bs = BaseSampler(train_ds, 1000)\n",
    "\n",
    "#YOUR CODE\n",
    "#Initialize your DataLoader (using datasets)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=bs)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: loss=0.39014437375590205 acc=0.87\n",
      "train: loss=0.4030041701346636 acc=0.868\n",
      "test: loss=0.3727508041076362 acc=0.88\n",
      "train: loss=0.39745164829306306 acc=0.857\n",
      "test: loss=0.3387663502711803 acc=0.885\n",
      "train: loss=0.3661242724601179 acc=0.881\n",
      "test: loss=0.33308697081170974 acc=0.87\n",
      "train: loss=0.347611439852044 acc=0.881\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader, print_info=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        collect_results = []\n",
    "        collect_target = []\n",
    "        for batch in loader:\n",
    "            X, y = batch\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE).detach().cpu().numpy()\n",
    "            pred = model(X)\n",
    "            collect_results.append(pred.sigmoid().detach().cpu().numpy())\n",
    "            collect_target.append(y) \n",
    "            \n",
    "    \n",
    "        preds_proba = np.concatenate(collect_results)\n",
    "        preds = preds_proba.argmax(axis=1)\n",
    "        \n",
    "        targets = np.concatenate(collect_target)\n",
    "\n",
    "        ll = log_loss(targets, preds_proba)\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        if print_info:\n",
    "            print(\"test log-loss: {}\".format(ll))\n",
    "            print(\"overall accuracy:  {}\".format(acc))\n",
    "            #print(classification_report(targets, preds))\n",
    "        model.train()\n",
    "        \n",
    "        return ll, acc\n",
    "    \n",
    "metrics = []\n",
    "metrics_names = ['loss_train','loss_test','acc_train','acc_test']\n",
    "losses = []\n",
    "net.to(DEVICE)\n",
    "\n",
    "\n",
    "W = writer.SummaryWriter(comment='resnet')\n",
    "\n",
    "\n",
    "for X,_ in train_dl:\n",
    "    break\n",
    "\n",
    "W.add_graph(net, X.cuda())\n",
    "\n",
    "step = 0\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    for name, param in net.named_parameters():\n",
    "        W.add_histogram(name, param.clone().cpu().data.numpy(), epoch) \n",
    "        \n",
    "    for X, y in train_dl:\n",
    "        step+=1\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        ypred=net(X)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        W.add_scalar('train/loss', loss.detach().data, step)\n",
    "\n",
    "        \n",
    "            \n",
    "    try:\n",
    "        for name, param in net.named_parameters():\n",
    "            W.add_histogram(name + 'grad', param.grad.clone().cpu().data.numpy(), epoch)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "    testll, testacc = evaluate_model(net, test_dl)\n",
    "    trainll, trainacc = evaluate_model(net, train_dl)\n",
    "    print(\"test: loss={} acc={}\".format(testll, testacc))\n",
    "    print(\"train: loss={} acc={}\".format(trainll, trainacc))\n",
    "    metrics.append([trainll, testll, trainacc, testacc])\n",
    "    \n",
    "    W.add_scalar('testacc', testacc, epoch)\n",
    "    W.add_scalar('trainacc', trainacc, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,10))\n",
    "pd.DataFrame(metrics, columns=['train-loss','test-loss','train-acc','test-acc']).plot(subplots=False, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercises\n",
    "\n",
    "1) Write a different Sampler for example one that will oversample the sneaker class by a factor of 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
