{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "from utils.vis import calculate_acc, plot_decision_space, visualize_data\n",
    "\n",
    "\n",
    "RANDOM_STATE = 45\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# DONT WORRY ABOUT THIS CODE VISUALIZATION ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 2\n",
    "\n",
    "X, Y = make_blobs(centers=2, random_state=RANDOM_STATE, n_features=N_FEATURES)\n",
    "X, Y = make_moons(n_samples=1000)\n",
    "X = X/np.abs(X).max()\n",
    "visualize_data(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X, columns=['x1','x2'])\n",
    "data['target'] = Y\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the perceptron\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*-JtN9TWuoZMz7z9QKbT85A.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_tensor = torch.randn((1,2))\n",
    "\n",
    "linear_layer = nn.Linear(2, 1, bias=False)\n",
    "\n",
    "linear_layer(input_tensor)\n",
    "\n",
    "print(linear_layer.weight)\n",
    "print(input_tensor)\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "\n",
    "print(input_tensor.shape)\n",
    "manual_output = input_tensor.mm(linear_layer.weight.transpose(0, 1))\n",
    "print(manual_output)\n",
    "assert output == manual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10, bias=True)\n",
    "        self.fc2 = nn.Linear(10, 10, bias=True)\n",
    "        self.fc3 = nn.Linear(10, 10, bias=True)\n",
    "        self.fc4 = nn.Linear(10, 1, bias=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc4(self.fc3(F.relu(self.fc2(F.relu(self.fc1(x))))))\n",
    "\n",
    "def custom_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "#         m.weight.fill_(0)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "perceptron = Perceptron(2)\n",
    "\n",
    "# Linear layers get their weights initialized by default, but you can reinitialize them if needed.\n",
    "perceptron.apply(custom_weights)\n",
    "\n",
    "# print(\"fc weight\", perceptron.fc.weight)\n",
    "# print(\"bias weight\", perceptron.fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in dir(perceptron) if not p.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop Using CrossEntropy\n",
    "\n",
    "\n",
    "\n",
    "While using crossentropy loss our model needs to return (BS, n_classes) output tensor, the target has to be a coresponding dense label vector of shape (BS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(net.pa)\n",
    "net = Perceptron(2)\n",
    "list(net.parameters())\n",
    "optimizer = SGD(net.parameters(), lr=0.01)\n",
    "optimizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "# init the model, loss and optimizer\n",
    "net = Perceptron(2)\n",
    "\n",
    "optimizer = SGD(net.parameters(), lr=0.1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "net = net.to(DEVICE)\n",
    "\n",
    "loss_history = []\n",
    "# neural_network.fit(X, y)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(\"training epoch {}\".format(epoch))\n",
    "    \n",
    "    for xsample,ysample in zip(X, Y):\n",
    "        optimizer.zero_grad() \n",
    "        # zero the gradients\n",
    "        \n",
    "        # batch preparation\n",
    "\n",
    "#         x = torch.Tensor(xsample).unsqueeze(0) # tensor([[0.8745, 0.5205]]) torch.Size([1, 2])\n",
    "#         y = torch.Tensor([ysample]).unsqueeze(0) # tensor([[1.]]) torch.Size([1, 1])\n",
    "#         x = x.to(DEVICE)\n",
    "#         y = y.to(DEVICE)\n",
    "        \n",
    "        if True:\n",
    "            ix = np.random.randint(0, size=(4,), high=len(X))\n",
    "            x = torch.tensor((X[ix, :])).float()\n",
    "            y = torch.tensor(torch.Tensor(Y[ix]).reshape(-1,1)).float()\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "        \n",
    "        # forward pass\n",
    "        out = net(x)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        # calculating gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # changing the weights by specified(in the optimizer init) learning rate\n",
    "        optimizer.step()\n",
    "        \n",
    "        #append calculated loss to the history\n",
    "        loss_history.append(loss.detach().cpu().numpy())\n",
    "    calculate_acc(net, X, Y)\n",
    "        \n",
    "#     plot_decision_boundry(net, X)\n",
    "# at the end plot final solution in red\n",
    "# plot_decision_boundry(net, X, 'r-')\n",
    "seaborn.scatterplot(x='x1',y='x2', hue='target', data=data)\n",
    "plt.xlim((-1, 1))\n",
    "plt.ylim((-1, 1))\n",
    "\n",
    "plot_decision_space(net, X, Y)\n",
    "        \n",
    "print(\"input shape (BS, n_classes):\", x.shape)\n",
    "print(\"target shape (BS, 1):\", y.shape)\n",
    "print(\"output shape\", out.shape)\n",
    "\n",
    "# def score_model(net, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(net, X, Y):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "\n",
    "    Xtorch = torch.from_numpy(X).float().cuda()\n",
    "    ytorch = torch.from_numpy(Y).float().cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = net(Xtorch)\n",
    "    predictions = (predictions.sigmoid() > .5).squeeze(-1)\n",
    "    predictions = np.array(predictions.cpu())\n",
    "    \n",
    "    real = np.array(ytorch.cpu())\n",
    "    acc = sum(predictions==real)/len(predictions)\n",
    "    print(\"model acc is: \", acc)\n",
    "    \n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(loss_history, columns=['loss'])\n",
    "history.loss[:].rolling(10).mean().plot()\n",
    "plt.title(\"loss\")\n",
    "plt.xlabel(\"batch number\")\n",
    "plt.ylabel(\"loss (CE)\")\n",
    "\n",
    "# history[::10].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1) Play with the training loop, enojoy the fact that you can inspect all the values dynamically. Consider using pdb.set_trace() for instance  \n",
    "2) Can you edit the Perceptron class to create a Multi Layer Perceptron? (ie having more then 0 hidden layers)  \n",
    "3) Initialize the the initial weights to 0. What do you think will happen? Can we still train the perceptron?  \n",
    "4) What kind of gradient descnet are we using here? Stochastic? Batch? or Vanilla?  \n",
    "5) What does detach do and why do we have to call it? (use google)  \n",
    "6) Try adding a RELU activation after Linear unit - Will it train?\n",
    "7) Try to implement a progress-bar (it might come in handy in our future exercises to)  \n",
    "8) We can see the loss never going to 0, but the accuracy probably is reaching 100% - calculate ACC each epoch\n",
    "9) Implement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.19214&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false width=1000 height=600></iframe>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
