{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from easyimages import EasyImageList\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from skorch import *\n",
    "from skorch import NeuralNet, NeuralNetClassifier\n",
    "from skorch.callbacks import (\n",
    "    Callback,\n",
    "    CyclicLR,\n",
    "    Freezer,\n",
    "    LRScheduler,\n",
    "    PrintLog,\n",
    "    scoring,\n",
    ")\n",
    "from skorch.callbacks.lr_scheduler import CyclicLR\n",
    "from skorch.callbacks.scoring import EpochScoring\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.utils import *\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the new dataset\n",
    "\n",
    "Keep in mind we are inheriting the Dataset base-class. And we have to override 2 methods:\n",
    " - \\____len____\n",
    " - \\____getitem____\n",
    "\n",
    "1. load_image_and_target reads the data from the csv file and converts the image array into a PIL object - this is not mandatory but useful, as many handy functions like transform, operate on PIL.Images\n",
    "2. getitem, takes a index and it should properly return for every number between (0, and len)\n",
    "3. len should return the number of samples (images) in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: 'T-shirt/top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "class FashionMnist(Dataset):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __init__(self, metadata_df,\n",
    "                 transform=None):\n",
    "        \n",
    "        self.metadata_df = metadata_df.copy()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.load_image_and_target(index)\n",
    "        # We can transform the output images here, cast to torch data-format and/or do augmentations\n",
    "        X = self.transform(X)\n",
    "            \n",
    "        return X, y\n",
    "    \n",
    "    def load_image_and_target(self,index):\n",
    "        # .iloc is short for integer loc it returns a row of data based on its ored not index-value(if not the same)\n",
    "        oneimage = self.metadata_df.iloc[index]\n",
    "        image, y = PIL.Image.fromarray(\n",
    "            np.array(oneimage[1:]).reshape(28, 28).astype('uint8'), 'L'), oneimage[0]\n",
    "        \n",
    "        return image, y\n",
    "    \n",
    "    def _load_image_metadata(self, index):\n",
    "        \n",
    "        image_path = df.iloc[index]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        meta_data = image.meta_data\n",
    "        \n",
    "        image, metada \n",
    "        \n",
    "\n",
    "\n",
    "    def collate_func(self, batch):\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "PATH_TO_FMNIST_TRAIN = './data/fashion-mnist_train.csv'\n",
    "PATH_TO_FMNIST_TEST = './data/fashion-mnist_test.csv'\n",
    "\n",
    "    \n",
    "dftrain = pd.read_csv(PATH_TO_FMNIST_TRAIN).sample(frac=1)\n",
    "dftest = pd.read_csv(PATH_TO_FMNIST_TEST).sample(frac=1)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.ToTensor()])\n",
    "fmnist_train = FashionMnist(dftrain, transform=transform_train)\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "fmnist_test = FashionMnist(dftest, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = fmnist_test[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f133784f470>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_image = dftrain.iloc[30].tolist()\n",
    "label, image =  one_sample_image[0], one_sample_image[1:]\n",
    "label\n",
    "im = np.array(image).reshape((28, 28))\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our datasets supports getitem wich means we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.int64'> 9\n",
      "<class 'PIL.Image.Image'> <PIL.Image.Image image mode=L size=28x28 at 0x7F133784F8D0>\n"
     ]
    }
   ],
   "source": [
    "image, label = fmnist_train[0]\n",
    "print(type(image))\n",
    "print(type(label), label)\n",
    "\n",
    "image, label = fmnist_train.load_image_and_target(0)\n",
    "print(type(image), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: torch.Size([32])\n",
      "X:  torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "fmnist_dl = DataLoader(fmnist_train, batch_size=32, num_workers=2)\n",
    "batch_iterator = iter(fmnist_dl)\n",
    "X, y = next(batch_iterator)\n",
    "\n",
    "for batch in fmnist_dl:\n",
    "    break\n",
    "for batch in fmnist_dl:\n",
    "    break\n",
    "    \n",
    "print(\"y:\",batch[1].shape)\n",
    "print(\"X: \", batch[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore your dataset\n",
    "Its always good to start with getting some inside about your dataset. Thats usuall much easier and less important in image datasets - but it does not mean you should not do it.\n",
    "\n",
    "Things you should always think about working with an image dataset are:\n",
    "- Check target distribiution (counts) do you see a strong imbalance?\n",
    "- Check sizes of images, if not all images are of the same size you might destroy your data with wrong transformations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f13379927b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize a batch\n",
    "Its always a good idea to sanity check the output of our loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = EasyImageList.from_torch_batch(X.repeat(1,3,1,1), # we need to add 3 channels to the images\n",
    "                                    mean=None, \n",
    "                                    std=None)\n",
    "im.numpy_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your model architecture\n",
    "As a baseline model dont  overcomplicate, either use some very simple architecture or go with something well established like resnet. There will be time to go more fancy\n",
    "\n",
    "Here we define a model with two CONV blocks comprising of a typical:  \n",
    "*Conv>BN>RELU>POOL* setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_of_class):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "    \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # we reduce the volume shape 2 times (2 Pooling Operations)\n",
    "        \n",
    "\n",
    "        self.fc = nn.Linear(int(28/2/2) * int(28/2/2) * 32, num_of_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\n",
      "torch.Size([32, 1, 28, 28])\n",
      "output shape:\n"
     ]
    }
   ],
   "source": [
    "# Always check your model are you atleasy able to make a forward pass and shapes match your expectations?\n",
    "image = torch.randn(32, 1, 28, 28)\n",
    "cnn = SimpleCNN(10)\n",
    "# out_layer2 = cnn.layer2(cnn.layer1(image))\n",
    "# out_layer2.reshape(out_layer2.size(0), -1).shape\n",
    "# output = cnn(image)\n",
    "print(\"input shape:\")\n",
    "print(image.shape)\n",
    "print(\"output shape:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your loss function / crieterion and optimizer\n",
    "Its a good idea to create some dummy data and pass it trough the cost function to make sure you uderstand what the shapes shoudl be\n",
    "\n",
    "In case of CrossEntropyLosss We want our predictions to be of shape (BATCH_SIZE, N_CLASSES) and y_true of shape (BS) \n",
    "(Basically a vector of number that corespond to the class) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6945)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR= 0.001\n",
    "BATCH_SIZE = 256\n",
    "DATASET_USAGE_SIZE = 0.05\n",
    "\n",
    "# MEAN = [0.485, 0.456, 0.406]\n",
    "# STD = [0.229, 0.224, 0.225]\n",
    "RESIZE = 28\n",
    "\n",
    "cnn = SimpleCNN(10)\n",
    "\n",
    "OPTIMIZER = 'Adam' # one of ['ASGD','Adadelta', 'Adagrad','Adam', 'Adamax','LBFGS', 'RMSprop','Rprop','SGD',SparseAdam']\n",
    "optimizer = getattr(torch.optim, OPTIMIZER)(cnn.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn.to(DEVICE)\n",
    "\n",
    "# Create dataset loaders\n",
    "\n",
    "dftrain = pd.read_csv(PATH_TO_FMNIST_TRAIN).sample(frac=DATASET_USAGE_SIZE)\n",
    "dftest = pd.read_csv(PATH_TO_FMNIST_TEST).sample(frac=0.1)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.Resize(RESIZE), transforms.ToTensor()])\n",
    "fmnist_train = FashionMnist(dftrain, transform=transform_train)\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(RESIZE), transforms.ToTensor()])\n",
    "fmnist_test = FashionMnist(dftest, transform=transform_test)\n",
    "\n",
    "fmnist_train_dl = DataLoader(fmnist_train, batch_size=BATCH_SIZE)\n",
    "fmnist_test_dl = DataLoader(fmnist_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Lets try to use the criterion with dummy data\n",
    "yp = torch.randn(BATCH_SIZE, 10)\n",
    "yt = torch.randint(10, (BATCH_SIZE,))\n",
    "criterion(yp, yt.long())\n",
    "\n",
    "# print(len(fmnist_train_dl))\n",
    "# print(len(dftrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: loss: 1.9908141578435898  acc: 0.685\n",
      "train: loss: 1.9934939339955648  acc: 0.6763333333333333\n",
      "test: loss: 1.3224473086893558  acc: 0.777\n",
      "train: loss: 1.322015470147133  acc: 0.756\n",
      "test: loss: 0.7701201834082604  acc: 0.782\n",
      "train: loss: 0.7495971059290071  acc: 0.7906666666666666\n",
      "test: loss: 0.5890897177426377  acc: 0.807\n",
      "train: loss: 0.5418179621524566  acc: 0.826\n",
      "test: loss: 0.5182543919299205  acc: 0.823\n",
      "train: loss: 0.44878554310814556  acc: 0.855\n",
      "test: loss: 0.48779840293785676  acc: 0.844\n",
      "train: loss: 0.40022504105603246  acc: 0.8713333333333333\n",
      "test: loss: 0.4683289269104789  acc: 0.851\n",
      "train: loss: 0.36442662599018755  acc: 0.8856666666666667\n",
      "test: loss: 0.453224176128715  acc: 0.852\n",
      "train: loss: 0.33510812951640156  acc: 0.8943333333333333\n",
      "test: loss: 0.4434790208442428  acc: 0.855\n",
      "train: loss: 0.310703485595391  acc: 0.903\n",
      "test: loss: 0.433574613996052  acc: 0.858\n",
      "train: loss: 0.2859996013590013  acc: 0.9133333333333333\n",
      "test: loss: 0.4265196909454253  acc: 0.861\n",
      "train: loss: 0.2635297033345687  acc: 0.917\n",
      "test: loss: 0.42206900740308995  acc: 0.862\n",
      "train: loss: 0.24409029446135114  acc: 0.9216666666666666\n",
      "test: loss: 0.4186393921013087  acc: 0.863\n",
      "train: loss: 0.22582383100749756  acc: 0.9293333333333333\n",
      "test: loss: 0.41661587597813693  acc: 0.868\n",
      "train: loss: 0.20987186358851506  acc: 0.9406666666666667\n",
      "test: loss: 0.4160918424163372  acc: 0.865\n",
      "train: loss: 0.19502311180444667  acc: 0.944\n",
      "test: loss: 0.4151710369924947  acc: 0.871\n",
      "train: loss: 0.18155134680789933  acc: 0.9476666666666667\n",
      "test: loss: 0.41749445493014103  acc: 0.87\n",
      "train: loss: 0.17066506737165787  acc: 0.955\n",
      "test: loss: 0.41791717145968893  acc: 0.877\n",
      "train: loss: 0.15817900121893194  acc: 0.9596666666666667\n",
      "test: loss: 0.41823788740777945  acc: 0.872\n",
      "train: loss: 0.1467903103643325  acc: 0.9636666666666667\n",
      "test: loss: 0.4136741856179142  acc: 0.879\n",
      "train: loss: 0.13253638201094164  acc: 0.9676666666666667\n",
      "test: loss: 0.4088927107692075  acc: 0.88\n",
      "train: loss: 0.11852205990792147  acc: 0.9756666666666667\n",
      "test: loss: 0.4100880601925137  acc: 0.876\n",
      "train: loss: 0.11079454270243332  acc: 0.9793333333333333\n",
      "test: loss: 0.4155690016940507  acc: 0.874\n",
      "train: loss: 0.11051151555588247  acc: 0.9806666666666667\n",
      "test: loss: 0.43409540397320845  acc: 0.861\n",
      "train: loss: 0.11896456840392489  acc: 0.972\n",
      "test: loss: 0.45195391061251305  acc: 0.858\n",
      "train: loss: 0.1226132432509277  acc: 0.9706666666666667\n"
     ]
    }
   ],
   "source": [
    "def process_train_batch(batch):\n",
    "    X, y = batch\n",
    "    return X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "def process_eval_batch(batch):\n",
    "    X, y = batch\n",
    "    X = X.to(DEVICE)\n",
    "    y = y.to(DEVICE).detach().cpu().numpy()   \n",
    "    return X, y \n",
    "\n",
    "def evaluate_model(model, test_loader, print_info=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        collect_results = []\n",
    "        collect_target = []\n",
    "        for batch in test_loader:\n",
    "            X, y = process_eval_batch(batch)\n",
    "            pred = model(X)\n",
    "            collect_results.append(F.softmax(pred, dim=1).detach().cpu().numpy())\n",
    "            collect_target.append(y) \n",
    "    \n",
    "        preds_proba = np.concatenate(collect_results)\n",
    "        preds = preds_proba.argmax(axis=1)\n",
    "\n",
    "        targets = np.concatenate(collect_target)\n",
    "        \n",
    "        ll = log_loss(targets, preds_proba)\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        if print_info:\n",
    "            print(\"test log-loss: {}\".format(ll))\n",
    "            print(\"overall accuracy:  {}\".format(ac))\n",
    "            #print(classification_report(targets, preds))\n",
    "        model.train()\n",
    "        return ll, acc\n",
    "    \n",
    "               \n",
    "collect_metrics = []\n",
    "collect_loss = []\n",
    "for epoch in range(25):\n",
    "    lossacc = 0\n",
    "    for i, batch in enumerate(fmnist_train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        X, y = process_train_batch(batch)\n",
    "        y_pred = cnn(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        collect_loss.append(float(loss.detach().cpu().numpy()))  \n",
    "        \n",
    "    lltest, acctest = evaluate_model(cnn, fmnist_test_dl)\n",
    "    lltrain, acctrain = evaluate_model(cnn, fmnist_train_dl)\n",
    "    collect_metrics.append([lltest, lltrain, acctest, acctrain])\n",
    "    print(\"test: loss: {}  acc: {}\".format(lltest, acctest))\n",
    "    print(\"train: loss: {}  acc: {}\".format(lltrain, acctrain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['lltest', 'lltrain', 'acctest', 'acctrain']\n",
    "\n",
    "metrics_df = pd.DataFrame(collect_metrics, columns=columns)\n",
    "plt.figure()\n",
    "metrics_df[['lltrain','lltest']].plot()\n",
    "plt.figure()\n",
    "metrics_df[['acctrain','acctest']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Exercises\n",
    "- why do we use torch.no_grad():\n",
    "- Try changing the kernel size of the CNN's what is your intuition should we use a smaller or bigger kernel?\n",
    "- Get the output of the first cnn, look at its shape and how it looks\n",
    "- Add dropout to the network, where does it make sense? To put it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crude LR-finder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(14.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "100\n",
      "tensor(2.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "200\n",
      "tensor(1.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "300\n",
      "tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "400\n",
      "tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "500\n",
      "tensor(2.7971, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "dftrain = pd.read_csv(PATH_TO_FMNIST_TRAIN).sample(frac=1)\n",
    "fmnist_train = FashionMnist(dftrain, transform=transform_train)\n",
    "data_loader = DataLoader(fmnist_train, batch_size=32)\n",
    "\n",
    "\n",
    "INITIAL_LR = 10e-7\n",
    "losses = []\n",
    "lrs = []\n",
    "cnn = SimpleCNN(10).cuda()\n",
    "optimizer = getattr(torch.optim, OPTIMIZER)(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = INITIAL_LR\n",
    "    \n",
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    if i % 100 == 0: \n",
    "        print(i)\n",
    "        print(loss)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    X, y = batch\n",
    "    X = X.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    y_pred = cnn(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if loss > 10:\n",
    "        break\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    losses.append(loss)\n",
    "    lrs.append(current_lr)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr * 1.03\n",
    "        \n",
    "    if i == 1000:\n",
    "        break\n",
    "\n",
    "\n",
    "    \n",
    "df = pd.DataFrame([l.detach().cpu().numpy() for l in losses])\n",
    "df['lrs'] = lrs\n",
    "ax = plt.plot(df['lrs'], df[0])\n",
    "plt.xscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
