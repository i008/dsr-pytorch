{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 10]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from retinanet.encoder import DataEncoder\n",
    "import imgaug as ia\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from retinanet.retinanet import RetinaNet\n",
    "from retinanet.loss import FocalLoss\n",
    "from utils.pikachu_dataset import load_data_pikachu\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load retinanet/encoder.py\n",
    "'''Encode object boxes and labels.'''\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from retinanet.utils import meshgrid, box_iou, box_nms, change_box_order\n",
    "\n",
    "\n",
    "class DataEncoder:\n",
    "    def __init__(self, anchor_areas=[32*32., 64*64., 128*128., 256*256., 512*512.]):\n",
    "        self.anchor_areas = anchor_areas   # p3 -> p7\n",
    "        self.aspect_ratios = [1/2., 1/1., 2/1.]\n",
    "        self.scale_ratios = [1., pow(2,1/3.), pow(2,2/3.)]\n",
    "        self.anchor_wh = self._get_anchor_wh()\n",
    "\n",
    "    def _get_anchor_wh(self):\n",
    "        '''Compute anchor width and height for each feature map.\n",
    "\n",
    "        Returns:\n",
    "          anchor_wh: (tensor) anchor wh, sized [#fm, #anchors_per_cell, 2].\n",
    "        '''\n",
    "        anchor_wh = []\n",
    "        for s in self.anchor_areas:\n",
    "            for ar in self.aspect_ratios:  # w/h = ar\n",
    "                h = math.sqrt(s/ar)\n",
    "                w = ar * h\n",
    "                for sr in self.scale_ratios:  # scale\n",
    "                    anchor_h = h*sr\n",
    "                    anchor_w = w*sr\n",
    "                    anchor_wh.append([anchor_w, anchor_h])\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        return torch.Tensor(anchor_wh).view(num_fms, -1, 2)\n",
    "\n",
    "    def _get_anchor_boxes(self, input_size):\n",
    "        '''Compute anchor boxes for each feature map.\n",
    "\n",
    "        Args:\n",
    "          input_size: (tensor) model input size of (w,h).\n",
    "\n",
    "        Returns:\n",
    "          boxes: (list) anchor boxes for each feature map. Each of size [#anchors,4],\n",
    "                        where #anchors = fmw * fmh * #anchors_per_cell\n",
    "        '''\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        fm_sizes = [(input_size/pow(2.,i+3)).ceil() for i in range(num_fms)]  # p3 -> p7 feature map sizes\n",
    "\n",
    "        boxes = []\n",
    "        for i in range(num_fms):\n",
    "            fm_size = fm_sizes[i]\n",
    "            grid_size = input_size / fm_size\n",
    "            fm_w, fm_h = int(fm_size[0]), int(fm_size[1])\n",
    "            xy = meshgrid(fm_w,fm_h) + 0.5 # [fm_h*fm_w, 2]\n",
    "            xy = (xy.float()*grid_size.float()).view(fm_h,fm_w,1,2).expand(fm_h,fm_w,9,2)\n",
    "            wh = self.anchor_wh[i].view(1,1,9,2).expand(fm_h,fm_w,9,2)\n",
    "           \n",
    "            \n",
    "            box = torch.cat([xy,wh], 3)  # [x,y,w,h]\n",
    "            boxes.append(box.view(-1,4))\n",
    "\n",
    "        return torch.cat(boxes, 0), boxes\n",
    "\n",
    "    def encode(self, boxes, labels, input_size):\n",
    "        '''Encode target bounding boxes and class labels.\n",
    "\n",
    "        We obey the Faster RCNN box coder:\n",
    "          tx = (x - anchor_x) / anchor_w\n",
    "          ty = (y - anchor_y) / anchor_h\n",
    "          tw = log(w / anchor_w)\n",
    "          th = log(h / anchor_h)\n",
    "\n",
    "        Args:\n",
    "          boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax), sized [#obj, 4].\n",
    "          labels: (tensor) object class labels, sized [#obj,].\n",
    "          input_size: (int/tuple) model input size of (w,h).\n",
    "\n",
    "        Returns:\n",
    "          loc_targets: (tensor) encoded bounding boxes, sized [#anchors,4].\n",
    "          cls_targets: (tensor) encoded class labels, sized [#anchors,].\n",
    "        '''\n",
    "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) \\\n",
    "                     else torch.Tensor(input_size)\n",
    "        anchor_boxes, _ = self._get_anchor_boxes(input_size)\n",
    "        boxes = change_box_order(boxes, 'xyxy2xywh')\n",
    "\n",
    "        ious = box_iou(anchor_boxes, boxes, order='xywh')\n",
    "        max_ious, max_ids = ious.max(1)\n",
    "        \n",
    "        boxes = boxes[max_ids]\n",
    "\n",
    "        loc_xy = (boxes[:,:2]-anchor_boxes[:,:2]) / anchor_boxes[:,2:]\n",
    "        loc_wh = torch.log(boxes[:,2:]/anchor_boxes[:,2:])\n",
    "        loc_targets = torch.cat([loc_xy,loc_wh], 1)\n",
    "        cls_targets = 1 + labels[max_ids]\n",
    "\n",
    "        cls_targets[max_ious<0.5] = 0\n",
    "        ignore = (max_ious>0.4) & (max_ious<0.5)  # ignore ious between [0.4,0.5]\n",
    "        cls_targets[ignore] = -1  # for now just mark ignored to -1\n",
    "        return loc_targets, cls_targets\n",
    "\n",
    "    def decode(self, loc_preds, cls_preds, input_size):\n",
    "        '''Decode outputs back to bouding box locations and class labels.\n",
    "\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [#anchors, 4].\n",
    "          cls_preds: (tensor) predicted class labels, sized [#anchors, #classes].\n",
    "          input_size: (int/tuple) model input size of (w,h).\n",
    "\n",
    "        Returns:\n",
    "          boxes: (tensor) decode box locations, sized [#obj,4].\n",
    "          labels: (tensor) class labels for each box, sized [#obj,].\n",
    "        '''\n",
    "        CLS_THRESH = 0.3\n",
    "        NMS_THRESH = 0.6\n",
    "\n",
    "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) \\\n",
    "                     else torch.Tensor(input_size)\n",
    "        anchor_boxes, _ = self._get_anchor_boxes(input_size)\n",
    "\n",
    "        loc_xy = loc_preds[:,:2]\n",
    "        loc_wh = loc_preds[:,2:]\n",
    "\n",
    "        xy = loc_xy * anchor_boxes[:,2:] + anchor_boxes[:,:2]\n",
    "        wh = loc_wh.exp() * anchor_boxes[:,2:]\n",
    "        boxes = torch.cat([xy-wh/2, xy+wh/2], 1)  # [#anchors,4]\n",
    "\n",
    "        score, labels = cls_preds.sigmoid().max(1)          # [#anchors,]\n",
    "        ids = score > CLS_THRESH\n",
    "        ids = ids.nonzero().squeeze()             # [#obj,]\n",
    "        keep = box_nms(boxes[ids], score[ids],labels, threshold=NMS_THRESH)\n",
    "        return boxes[ids][keep], labels[ids][keep], score[ids][keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinanet.loss import  * \n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def focal_loss(self, x, y):\n",
    "        '''Focal loss.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        alpha = 0.25\n",
    "        gamma = 2\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)  # [N,21]\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        t = t[:,1:]  # exclude background\n",
    "        t = Variable(t).cuda()  # [N,20]\n",
    "\n",
    "        p = x.sigmoid().detach()\n",
    "        pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
    "        w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
    "        w = w * (1-pt).pow(gamma)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
    "\n",
    "    def focal_loss_alt(self, x, y):\n",
    "        '''Focal loss alternative.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        alpha = 0.25\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)\n",
    "        t = t[:,1:]\n",
    "        t = Variable(t).cuda()\n",
    "\n",
    "        xt = x*(2*t-1)  # xt = x if t > 0 else -x\n",
    "        pt = (2*xt+1).sigmoid() \n",
    "\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        loss = -w*pt.log() / 2\n",
    "        return loss.sum()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
    "          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
    "          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
    "        loss:\n",
    "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
    "        '''\n",
    "        \n",
    "        loc_preds, cls_preds = pred\n",
    "        loc_targets, cls_targets = target\n",
    "\n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        batch_size, num_boxes = cls_targets.size()\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.data.long().sum()\n",
    "\n",
    "        ################################################################\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        ################################################################\n",
    "        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
    "        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
    "        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
    "        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
    "\n",
    "        ################################################################\n",
    "        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
    "        ################################################################\n",
    "        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
    "        num_peg = pos_neg.data.long().sum()\n",
    "        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
    "        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
    "        \n",
    "#         fl = FL()\n",
    "#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
    "\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
    "#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.data.long().sum()\n",
    "        num_pos_neg = pos_neg.data.long().sum()\n",
    "\n",
    "        if num_pos > 0:\n",
    "            loss = (cls_loss + loc_loss) / num_pos\n",
    "        elif num_pos_neg > 0:\n",
    "            loss = cls_loss\n",
    "        else:\n",
    "            raise Exception('num_pos_neg == 0')\n",
    "            \n",
    "        loss = loc_loss + cls_loss\n",
    "        return loss\n",
    "    \n",
    "class PikachuDataset(Dataset):\n",
    "    def __init__(self, anchor_areas=None):\n",
    "        \n",
    "        self.anchor_areas = anchor_areas\n",
    "        self.train, self.val = load_data_pikachu(1)\n",
    "        if anchor_areas is not None:\n",
    "            encoder = DataEncoder(anchor_areas=anchor_areas)\n",
    "        else:\n",
    "            encoder = DataEncoder()            \n",
    "        self.encoder = encoder \n",
    "        \n",
    "    def __len__(self):\n",
    "        return 900\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            b = self.train.next()\n",
    "        except StopIteration:\n",
    "            self.train.reset()\n",
    "            b = self.train.next()\n",
    "            \n",
    "        image = b.data[0].asnumpy()\n",
    "        bbox = b.label[0].asnumpy()[:, 0, 1:] * 256\n",
    "        label = b.label[0].asnumpy()[:, 0, 0]\n",
    "    \n",
    "        image, bbox, label = torch.from_numpy(image), torch.from_numpy(bbox), torch.from_numpy(label)   \n",
    "        \n",
    "        encoded = self.encoder.encode(bbox, label, torch.Tensor([256, 256]))\n",
    "        \n",
    "        loc_target, cls_target = encoded\n",
    "        \n",
    "        return (image / 255)[0], (loc_target, cls_target)\n",
    "        \n",
    "    \n",
    "    def collate_func(self, batch):\n",
    "\n",
    "        images = [b[0][0] for b in batch]\n",
    "        bbox = [b[1] for b in batch]\n",
    "        labels = [b[2] for b in batch]\n",
    "        \n",
    "        \n",
    "        encoded = [self.encoder.encode(bb, l, torch.Tensor([256, 256])) for bb, l in zip(bbox, labels)]\n",
    "        \n",
    "        \n",
    "        loc_target = [l[0] for l in encoded]\n",
    "        cls_target = [l[1] for l in encoded]\n",
    "        \n",
    "\n",
    "        return torch.stack(images) / 255, torch.stack(loc_target), torch.stack(cls_target)\n",
    "        \n",
    "        \n",
    "def down_sample(in_channels,out_channels):\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)       \n",
    "    )\n",
    "\n",
    "class SimpleSSD(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_cls=1, num_anchors=9):\n",
    "        super(SimpleSSD, self).__init__()\n",
    "        \n",
    "        self.n_cls = n_cls\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        \n",
    "        # Base CNN (think resnet/vgg or other base network)\n",
    "        self.step1 = down_sample(3, 128)\n",
    "        self.step2 = down_sample(128, 128)\n",
    "        self.step3 = down_sample(128, 128)\n",
    "        self.step4 = down_sample(128, 128)\n",
    "        self.step5 = down_sample(128, 128)\n",
    "\n",
    "\n",
    "        self.cls_head1 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head1 = nn.Conv2d(128, self.num_anchors *4, 3, padding=1)\n",
    "        \n",
    "        self.cls_head2 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head2 = nn.Conv2d(128,self.num_anchors * 4,3, padding=1)\n",
    "        \n",
    "        self.cls_head3 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head3 = nn.Conv2d(128, self.num_anchors * 4,3, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        step1=self.step1(x)\n",
    "        step2=self.step2(step1)\n",
    "\n",
    "        step3=self.step3(step2)\n",
    "        step4 = self.step4(step3)\n",
    "        step5 = self.step5(step4)\n",
    "                \n",
    "        cls1 = self.cls_head1(step3)\n",
    "        bbox1 = self.bbox_head1(step3)\n",
    "        \n",
    "        cls2 = self.cls_head2(step4)\n",
    "        bbox2 = self.bbox_head2(step4)\n",
    "        \n",
    "        cls3 = self.cls_head3(step5)\n",
    "        bbox3 = self.bbox_head3(step5) \n",
    "\n",
    "    \n",
    "        cls1 = cls1.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        cls2 = cls2.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        cls3 = cls3.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        \n",
    "        bbox1 = bbox1.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox2 = bbox2.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox3 = bbox3.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "      \n",
    "        cls_pred = torch.cat([cls1, cls2, cls3], dim=1)\n",
    "        bbox_pred = torch.cat([bbox1, bbox2, bbox3], dim=1)\n",
    "        \n",
    "        return (bbox_pred, cls_pred)\n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pikachu_ds =PikachuDataset(anchor_areas=[30*30, 50*50, 90*90])\n",
    "pikachu_dl = DataLoader(pikachu_ds, batch_size=2, collate_fn=pikachu_ds.collate_func)\n",
    "\n",
    "# for b in pikachu_dl:\n",
    "#     break\n",
    "# import pandas as pd  \n",
    "# b[0].shape\n",
    "# b[1].shape\n",
    "# pd.DataFrame(b[2][0].cpu().numpy())[0].value_counts()\n",
    "# # b[0].shape\n",
    "\n",
    "# for b in pikachu_dl:\n",
    "#     break\n",
    "    \n",
    "# # b[0].shape\n",
    "# pikachu_ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SimpleSSD(1)\n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(num_classes=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# collect = []\n",
    "\n",
    "\n",
    "# for epoch in range(2): \n",
    "#     for i, b in enumerate(pikachu_dl):\n",
    "#         logs = {}\n",
    "#         optimizer.zero_grad()\n",
    "#         image, bounding_boxes, labels = b\n",
    "#         image = image.to(device)\n",
    "#         bounding_boxes = bounding_boxes.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         loc_pred, cls_pred = model(image)\n",
    "#         total_loss = criterion(loc_pred, bounding_boxes, cls_pred, labels)\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         collect.append([total_loss.detach().cpu().numpy()])\n",
    "#         logs['loss'] = total_loss.item()\n",
    "#         if i % 10 == 0:\n",
    "#             plot_losses.update(logs)\n",
    "#             plot_losses.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-2-60f37db5d825>(90)encode()\n",
      "-> boxes = boxes[max_ids]\n",
      "(Pdb) ious\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "(Pdb) ious.shape\n",
      "torch.Size([12096, 1])\n",
      "(Pdb) l\n",
      " 85  \t        max_ious, max_ids = ious.max(1)\n",
      " 86  \t\n",
      " 87  \t        import pdb\n",
      " 88  \t        pdb.set_trace()\n",
      " 89  \t\n",
      " 90  ->\t        boxes = boxes[max_ids]\n",
      " 91  \t\n",
      " 92  \t        loc_xy = (boxes[:,:2]-anchor_boxes[:,:2]) / anchor_boxes[:,2:]\n",
      " 93  \t        loc_wh = torch.log(boxes[:,2:]/anchor_boxes[:,2:])\n",
      " 94  \t        loc_targets = torch.cat([loc_xy,loc_wh], 1)\n",
      " 95  \t        cls_targets = 1 + labels[max_ids]\n",
      "(Pdb) max_ious.shape\n",
      "torch.Size([12096])\n",
      "(Pdb) max_ious.sum()\n",
      "tensor(209.0254)\n",
      "(Pdb) max_ious[-1]\n",
      "tensor(0.)\n",
      "(Pdb) max_ious\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0e94678c92a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 batch_size=8, device='cuda')\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpikachu_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_train_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_epoch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mon_epoch_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m                 \u001b[0myi_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my_train_is_ph\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myi_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-28c495cdffb9>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mloc_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-60f37db5d825>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, boxes, labels, input_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mloc_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0manchor_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0manchor_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-60f37db5d825>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, boxes, labels, input_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mloc_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0manchor_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0manchor_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skorch.net import NeuralNet\n",
    "\n",
    "net = NeuralNet(model,\n",
    "                criterion=FocalLoss, \n",
    "                criterion__num_classes=1,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                lr=0.0001,\n",
    "                batch_size=8, device='cuda')\n",
    "\n",
    "net.fit(pikachu_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = net.forward(pikachu_ds)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dc9d24a1c5db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToPILImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpikachu_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-577a68198978>\u001b[0m in \u001b[0;36mcollate_func\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-577a68198978>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize, ToPILImage, ToTensor\n",
    "it = iter(pikachu_dl)\n",
    "b = next(it)\n",
    "\n",
    "im = ToTensor()(Resize((256, 256))(ToPILImage()(torch.cat([b[0][0], b[0][0]], dim=1))))\n",
    "image = im.unsqueeze(0)\n",
    "\n",
    "enc = DataEncoder(anchor_areas=[30*30, 50*50, 90*90])\n",
    "loc_pred, cls_pred = model(image.cuda())\n",
    "\n",
    "# i = 0\n",
    "# bbspred, labelpred, score  = enc.decode(\n",
    "#     loc_pred[i].float().cpu(), \n",
    "#     cls_pred[i].float().cpu(), \n",
    "#     torch.Tensor([256, 256]).float().cpu()\n",
    "# )\n",
    "\n",
    "# image_to_show = np.moveaxis(\n",
    "#     image[i].detach().cpu().numpy(),0, 2)\n",
    "\n",
    "# matched_anchors_on_image = ia.BoundingBoxesOnImage(\n",
    "#     [ia.BoundingBox(*b) for b in bbspred.detach().cpu().numpy()], shape=(256, 256))\n",
    "\n",
    "# image_to_show = matched_anchors_on_image.draw_on_image(image_to_show, thickness=2)\n",
    "# plt.imshow(image_to_show)\n",
    "# plt.title('score ' + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Load a custom image with pikachu( or many ) and try to make predictiosn with the network and visualize the result\n",
    "-  Can you think of anything that could confuse our detector? yellow dots ?\n",
    "- Currently the code is not really modular, try to make it nice by splitting it into logical parts\n",
    "    - Base feature extractor Module\n",
    "    - Head Creator module\n",
    "- Currently the detection/cls HEADS are very simple (just one CONV layer) they can be more complex. Try using more convolutions, check other architectures how its done\n",
    "\n",
    "- Can you use our network to train using some new data for instance:\n",
    "    - https://www.kaggle.com/tomluther/ships-in-google-earth\n",
    "    - https://www.kaggle.com/aruchomu/data-for-yolo-v3-kernel\n",
    "    - https://www.kaggle.com/dataturks/face-detection-in-images\n",
    "    - https://www.kaggle.com/dataturks/face-dataset-with-age-emotion-ethnicity  \n",
    " You will need to create a data loader/data sets similar as we did for the pikachu loader. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_sigmoid_focal_loss(pred,\n",
    "                          target,\n",
    "                          weight,\n",
    "                          gamma=2.0,\n",
    "                          alpha=0.25,\n",
    "                          reduction='mean'):\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    target = target.type_as(pred)\n",
    "    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "    weight = (alpha * target + (1 - alpha) * (1 - target)) * weight\n",
    "    weight = weight * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, target, reduction='none') * weight\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()\n",
    "    \n",
    "    \n",
    "def focal_loss(x, y, num_class=2):\n",
    "    '''Focal loss.\n",
    "    Args:\n",
    "      x: (tensor) sized [N,D].\n",
    "      y: (tensor) sized [N,].\n",
    "    Return:\n",
    "      (tensor) focal loss.\n",
    "    '''\n",
    "    alpha = 0.25\n",
    "    gamma = 2\n",
    "\n",
    "    t = one_hot_embedding(y.data.cpu(), num_class)  # [N,21]\n",
    "#     t = t[:,1:]  # exclude background\n",
    "#     t = Variable(t).cuda()  # [N,20]\n",
    "\n",
    "\n",
    "    p = x.sigmoid().detach()\n",
    "    pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
    "    w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
    "    w = w * (1-pt).pow(gamma)\n",
    "    return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1840)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([0, 0.1, 0, 1.0])\n",
    "target = torch.tensor([0,1,1,0])\n",
    "\n",
    "py_sigmoid_focal_loss(pred, target, weight=torch.tensor([1.0,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0433)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([10, 2])\n",
    "y = torch.rand([10, 2]).round().int().float()\n",
    "x\n",
    "\n",
    "x = torch.tensor([[10000.0, 0.0, 10]])\n",
    "y = torch.tensor([[1, 0.0, 1]])\n",
    "\n",
    "py_sigmoid_focal_loss(x, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.stack([torch.ones_like(pred) - pred, pred]).view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-99ef6afa34d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-221-ca8ef2645337>\u001b[0m in \u001b[0;36mfocal_loss\u001b[0;34m(x, y, n)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# pt = p if t > 0 else 1-p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# w = alpha if t > 0 else 1-alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "target = target.reshape(-1, 1)\n",
    "focal_loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([1])\n",
      "> <ipython-input-254-ff27f5bc0c68>(41)focal_loss()\n",
      "-> p = x.sigmoid().detach()\n",
      "(Pdb) l\n",
      " 36  \t    t = t[:,1:]  # exclude background\n",
      " 37  \t#     t = Variable(t).cuda()  # [N,20]\n",
      " 38  \t    import pdb\n",
      " 39  \t    pdb.set_trace()\n",
      " 40  \t\n",
      " 41  ->\t    p = x.sigmoid().detach()\n",
      " 42  \t    pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
      " 43  \t    w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
      " 44  \t    w = w * (1-pt).pow(gamma)\n",
      " 45  \t    return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "[EOF]\n",
      "(Pdb) x.shape\n",
      "torch.Size([1, 3])\n",
      "(Pdb) y.shape\n",
      "torch.Size([1])\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(42)focal_loss()\n",
      "-> pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(43)focal_loss()\n",
      "-> w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(44)focal_loss()\n",
      "-> w = w * (1-pt).pow(gamma)\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(45)focal_loss()\n",
      "-> return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "(Pdb) n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))\n",
      "> <ipython-input-254-ff27f5bc0c68>(45)focal_loss()\n",
      "-> return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-254-ff27f5bc0c68>(45)focal_loss()->None\n",
      "-> return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "(Pdb) n\n",
      "ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))\n",
      "> <ipython-input-255-fb104846a17c>(5)<module>()\n",
      "-> focal_loss(x, y, n=3)\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-255-fb104846a17c>(5)<module>()->None\n",
      "-> focal_loss(x, y, n=3)\n",
      "(Pdb) n\n",
      "ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))\n",
      "> /home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2961)run_code()\n",
      "-> exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "(Pdb) n\n",
      "> /home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2964)run_code()\n",
      "-> sys.excepthook = old_excepthook\n",
      "(Pdb) n\n",
      "> /home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2965)run_code()\n",
      "-> except SystemExit as e:\n",
      "(Pdb) c\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-fb104846a17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-254-ff27f5bc0c68>\u001b[0m in \u001b[0;36mfocal_loss\u001b[0;34m(x, y, n)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# w = alpha if t > 0 else 1-alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Pdb) masked_cls_preds.shape\n",
    "torch.Size([96382, 1])\n",
    "(Pdb) cls_targets[pos_neg].shape\n",
    "torch.Size([96382])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.9, 0, 0]])\n",
    "y = torch.tensor([0])\n",
    "\n",
    "\n",
    "# t = one_hot_embedding(torch.tensor(y), 3) \n",
    "# t = one_hot_embedding(y, 1+1) # [N,21]\n",
    "# print(t)\n",
    "# t = t[:,1:]  # exclude background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-272-2febcda2fe0e>(41)focal_loss()\n",
      "-> p = x.sigmoid().detach()\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2671)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(x, y, num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def box_iou(box1, box2, order='xyxy'):\n",
    "    '''Compute the intersection over union of two set of boxes.\n",
    "\n",
    "    The default box order is (xmin, ymin, xmax, ymax).\n",
    "\n",
    "    Args:\n",
    "      box1: (tensor) bounding boxes, sized [N,4].\n",
    "      box2: (tensor) bounding boxes, sized [M,4].\n",
    "      order: (str) box order, either 'xyxy' or 'xywh'.\n",
    "\n",
    "    Return:\n",
    "      (tensor) iou, sized [N,M].\n",
    "\n",
    "    Reference:\n",
    "      https://github.com/chainer/chainercv/blob/master/chainercv/utils/bbox/bbox_iou.py\n",
    "    '''\n",
    "    if order == 'xywh':\n",
    "        box1 = change_box_order(box1, 'xywh2xyxy')\n",
    "        box2 = change_box_order(box2, 'xywh2xyxy')\n",
    "\n",
    "    N = box1.size(0)\n",
    "    M = box2.size(0)\n",
    "\n",
    "    lt = torch.max(box1[:,None,:2], box2[:,:2])  # [N,M,2]\n",
    "    rb = torch.min(box1[:,None,2:], box2[:,2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb-lt+1).clamp(min=0)      # [N,M,2]\n",
    "    inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n",
    "\n",
    "    area1 = (box1[:,2]-box1[:,0]+1) * (box1[:,3]-box1[:,1]+1)  # [N,]\n",
    "    area2 = (box2[:,2]-box2[:,0]+1) * (box2[:,3]-box2[:,1]+1)  # [M,]\n",
    "    iou = inter / (area1[:,None] + area2 - inter)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = torch.tensor([[10.0,10.0,20,20],[8,8,15,15], [3,3,10,10], [9,9,20,20], [30,30,40,40]])\n",
    "b2 = torch.tensor([[10.0, 10,23, 20],[12,12,30,30], [28,28,35, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = box_iou(b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7857, 0.2020, 0.0000],\n",
       "        [0.1978, 0.0391, 0.0000],\n",
       "        [0.0046, 0.0000, 0.0000],\n",
       "        [0.6836, 0.1910, 0.0000],\n",
       "        [0.0000, 0.0021, 0.2416]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, ix = ious.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 23., 20.],\n",
       "        [10., 10., 23., 20.],\n",
       "        [10., 10., 23., 20.],\n",
       "        [10., 10., 23., 20.],\n",
       "        [28., 28., 35., 35.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 20., 20.],\n",
       "        [ 8.,  8., 15., 15.],\n",
       "        [ 3.,  3., 10., 10.],\n",
       "        [ 9.,  9., 20., 20.],\n",
       "        [30., 30., 40., 40.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
