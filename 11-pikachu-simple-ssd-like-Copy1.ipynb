{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 10]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from retinanet.encoder import DataEncoder\n",
    "import imgaug as ia\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from retinanet.retinanet import RetinaNet\n",
    "from retinanet.loss import FocalLoss\n",
    "from utils.pikachu_dataset import load_data_pikachu\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load retinanet/encoder.py\n",
    "'''Encode object boxes and labels.'''\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from retinanet.utils import meshgrid, box_iou, box_nms, change_box_order\n",
    "\n",
    "\n",
    "class DataEncoder:\n",
    "    def __init__(self, anchor_areas=[32*32., 64*64., 128*128., 256*256., 512*512.]):\n",
    "        self.anchor_areas = anchor_areas   # p3 -> p7\n",
    "        self.aspect_ratios = [1/2., 1/1., 2/1.]\n",
    "        self.scale_ratios = [1., pow(2,1/3.), pow(2,2/3.)]\n",
    "        self.anchor_wh = self._get_anchor_wh()\n",
    "\n",
    "    def _get_anchor_wh(self):\n",
    "        '''Compute anchor width and height for each feature map.\n",
    "\n",
    "        Returns:\n",
    "          anchor_wh: (tensor) anchor wh, sized [#fm, #anchors_per_cell, 2].\n",
    "        '''\n",
    "        anchor_wh = []\n",
    "        for s in self.anchor_areas:\n",
    "            for ar in self.aspect_ratios:  # w/h = ar\n",
    "                h = math.sqrt(s/ar)\n",
    "                w = ar * h\n",
    "                for sr in self.scale_ratios:  # scale\n",
    "                    anchor_h = h*sr\n",
    "                    anchor_w = w*sr\n",
    "                    anchor_wh.append([anchor_w, anchor_h])\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        return torch.Tensor(anchor_wh).view(num_fms, -1, 2)\n",
    "\n",
    "    def _get_anchor_boxes(self, input_size):\n",
    "        '''Compute anchor boxes for each feature map.\n",
    "\n",
    "        Args:\n",
    "          input_size: (tensor) model input size of (w,h).\n",
    "\n",
    "        Returns:\n",
    "          boxes: (list) anchor boxes for each feature map. Each of size [#anchors,4],\n",
    "                        where #anchors = fmw * fmh * #anchors_per_cell\n",
    "        '''\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        fm_sizes = [(input_size/pow(2.,i+3)).ceil() for i in range(num_fms)]  # p3 -> p7 feature map sizes\n",
    "\n",
    "        boxes = []\n",
    "        for i in range(num_fms):\n",
    "            fm_size = fm_sizes[i]\n",
    "            grid_size = input_size / fm_size\n",
    "            fm_w, fm_h = int(fm_size[0]), int(fm_size[1])\n",
    "            xy = meshgrid(fm_w,fm_h) + 0.5 # [fm_h*fm_w, 2]\n",
    "            xy = (xy.float()*grid_size.float()).view(fm_h,fm_w,1,2).expand(fm_h,fm_w,9,2)\n",
    "            wh = self.anchor_wh[i].view(1,1,9,2).expand(fm_h,fm_w,9,2)\n",
    "           \n",
    "            \n",
    "            box = torch.cat([xy,wh], 3)  # [x,y,w,h]\n",
    "            boxes.append(box.view(-1,4))\n",
    "\n",
    "        return torch.cat(boxes, 0), boxes\n",
    "\n",
    "    def encode(self, boxes, labels, input_size):\n",
    "        '''Encode target bounding boxes and class labels.\n",
    "\n",
    "        We obey the Faster RCNN box coder:\n",
    "          tx = (x - anchor_x) / anchor_w\n",
    "          ty = (y - anchor_y) / anchor_h\n",
    "          tw = log(w / anchor_w)\n",
    "          th = log(h / anchor_h)\n",
    "\n",
    "        Args:\n",
    "          boxes: (tensor) bounding boxes of (xmin,ymin,xmax,ymax), sized [#obj, 4].\n",
    "          labels: (tensor) object class labels, sized [#obj,].\n",
    "          input_size: (int/tuple) model input size of (w,h).\n",
    "\n",
    "        Returns:\n",
    "          loc_targets: (tensor) encoded bounding boxes, sized [#anchors,4].\n",
    "          cls_targets: (tensor) encoded class labels, sized [#anchors,].\n",
    "        '''\n",
    "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) \\\n",
    "                     else torch.Tensor(input_size)\n",
    "        anchor_boxes, _ = self._get_anchor_boxes(input_size)\n",
    "        boxes = change_box_order(boxes, 'xyxy2xywh')\n",
    "\n",
    "        ious = box_iou(anchor_boxes, boxes, order='xywh')\n",
    "        max_ious, max_ids = ious.max(1)\n",
    "        \n",
    "        boxes = boxes[max_ids]\n",
    "\n",
    "        loc_xy = (boxes[:,:2]-anchor_boxes[:,:2]) / anchor_boxes[:,2:]\n",
    "        loc_wh = torch.log(boxes[:,2:]/anchor_boxes[:,2:])\n",
    "        loc_targets = torch.cat([loc_xy,loc_wh], 1)\n",
    "        cls_targets = 1 + labels[max_ids]\n",
    "\n",
    "        cls_targets[max_ious<0.5] = 0\n",
    "        ignore = (max_ious>0.4) & (max_ious<0.5)  # ignore ious between [0.4,0.5]\n",
    "        cls_targets[ignore] = -1  # for now just mark ignored to -1\n",
    "        return loc_targets, cls_targets\n",
    "\n",
    "    def decode(self, loc_preds, cls_preds, input_size):\n",
    "        '''Decode outputs back to bouding box locations and class labels.\n",
    "\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [#anchors, 4].\n",
    "          cls_preds: (tensor) predicted class labels, sized [#anchors, #classes].\n",
    "          input_size: (int/tuple) model input size of (w,h).\n",
    "\n",
    "        Returns:\n",
    "          boxes: (tensor) decode box locations, sized [#obj,4].\n",
    "          labels: (tensor) class labels for each box, sized [#obj,].\n",
    "        '''\n",
    "        CLS_THRESH = 0.3\n",
    "        NMS_THRESH = 0.6\n",
    "\n",
    "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) \\\n",
    "                     else torch.Tensor(input_size)\n",
    "        anchor_boxes, _ = self._get_anchor_boxes(input_size)\n",
    "\n",
    "        loc_xy = loc_preds[:,:2]\n",
    "        loc_wh = loc_preds[:,2:]\n",
    "\n",
    "        xy = loc_xy * anchor_boxes[:,2:] + anchor_boxes[:,:2]\n",
    "        wh = loc_wh.exp() * anchor_boxes[:,2:]\n",
    "        boxes = torch.cat([xy-wh/2, xy+wh/2], 1)  # [#anchors,4]\n",
    "\n",
    "        score, labels = cls_preds.sigmoid().max(1)          # [#anchors,]\n",
    "        ids = score > CLS_THRESH\n",
    "        ids = ids.nonzero().squeeze()             # [#obj,]\n",
    "        keep = box_nms(boxes[ids], score[ids],labels, threshold=NMS_THRESH)\n",
    "        return boxes[ids][keep], labels[ids][keep], score[ids][keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinanet.loss import  * \n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def focal_loss(self, x, y):\n",
    "        '''Focal loss.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        alpha = 0.25\n",
    "        gamma = 2\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)  # [N,21]\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        t = t[:,1:]  # exclude background\n",
    "        t = Variable(t).cuda()  # [N,20]\n",
    "\n",
    "        p = x.sigmoid().detach()\n",
    "        pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
    "        w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
    "        w = w * (1-pt).pow(gamma)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
    "\n",
    "    def focal_loss_alt(self, x, y):\n",
    "        '''Focal loss alternative.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        alpha = 0.25\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)\n",
    "        t = t[:,1:]\n",
    "        t = Variable(t).cuda()\n",
    "\n",
    "        xt = x*(2*t-1)  # xt = x if t > 0 else -x\n",
    "        pt = (2*xt+1).sigmoid() \n",
    "\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        loss = -w*pt.log() / 2\n",
    "        return loss.sum()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
    "          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
    "          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
    "        loss:\n",
    "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
    "        '''\n",
    "        \n",
    "        loc_preds, cls_preds = pred\n",
    "        loc_targets, cls_targets = target\n",
    "\n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        batch_size, num_boxes = cls_targets.size()\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.data.long().sum()\n",
    "\n",
    "        ################################################################\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        ################################################################\n",
    "        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
    "        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
    "        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
    "        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
    "\n",
    "        ################################################################\n",
    "        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
    "        ################################################################\n",
    "        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
    "        num_peg = pos_neg.data.long().sum()\n",
    "        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
    "        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
    "        \n",
    "#         fl = FL()\n",
    "#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
    "\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
    "#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.data.long().sum()\n",
    "        num_pos_neg = pos_neg.data.long().sum()\n",
    "\n",
    "        if num_pos > 0:\n",
    "            loss = (cls_loss + loc_loss) / num_pos\n",
    "        elif num_pos_neg > 0:\n",
    "            loss = cls_loss\n",
    "        else:\n",
    "            raise Exception('num_pos_neg == 0')\n",
    "            \n",
    "        loss = loc_loss + cls_loss\n",
    "        return loss\n",
    "    \n",
    "class PikachuDataset(Dataset):\n",
    "    def __init__(self, anchor_areas=None):\n",
    "        \n",
    "        self.anchor_areas = anchor_areas\n",
    "        self.train, self.val = load_data_pikachu(1)\n",
    "        if anchor_areas is not None:\n",
    "            encoder = DataEncoder(anchor_areas=anchor_areas)\n",
    "        else:\n",
    "            encoder = DataEncoder()            \n",
    "        self.encoder = encoder \n",
    "        \n",
    "    def __len__(self):\n",
    "        return 900\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            b = self.train.next()\n",
    "        except StopIteration:\n",
    "            self.train.reset()\n",
    "            b = self.train.next()\n",
    "            \n",
    "        image = b.data[0].asnumpy()\n",
    "        bbox = b.label[0].asnumpy()[:, 0, 1:] * 256\n",
    "        label = b.label[0].asnumpy()[:, 0, 0]\n",
    "    \n",
    "        image, bbox, label = torch.from_numpy(image), torch.from_numpy(bbox), torch.from_numpy(label)   \n",
    "        \n",
    "        encoded = self.encoder.encode(bbox, label, torch.Tensor([256, 256]))\n",
    "        \n",
    "        loc_target, cls_target = encoded\n",
    "        \n",
    "        return (image / 255)[0], (loc_target, cls_target)\n",
    "        \n",
    "    \n",
    "    def collate_func(self, batch):\n",
    "\n",
    "        images = [b[0][0] for b in batch]\n",
    "        bbox = [b[1] for b in batch]\n",
    "        labels = [b[2] for b in batch]\n",
    "        \n",
    "        \n",
    "        encoded = [self.encoder.encode(bb, l, torch.Tensor([256, 256])) for bb, l in zip(bbox, labels)]\n",
    "        \n",
    "        \n",
    "        loc_target = [l[0] for l in encoded]\n",
    "        cls_target = [l[1] for l in encoded]\n",
    "        \n",
    "\n",
    "        return torch.stack(images) / 255, torch.stack(loc_target), torch.stack(cls_target)\n",
    "        \n",
    "        \n",
    "def down_sample(in_channels,out_channels):\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)       \n",
    "    )\n",
    "\n",
    "class SimpleSSD(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_cls=1, num_anchors=9):\n",
    "        super(SimpleSSD, self).__init__()\n",
    "        \n",
    "        self.n_cls = n_cls\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        \n",
    "        # Base CNN (think resnet/vgg or other base network)\n",
    "        self.step1 = down_sample(3, 128)\n",
    "        self.step2 = down_sample(128, 128)\n",
    "        self.step3 = down_sample(128, 128)\n",
    "        self.step4 = down_sample(128, 128)\n",
    "        self.step5 = down_sample(128, 128)\n",
    "\n",
    "\n",
    "        self.cls_head1 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head1 = nn.Conv2d(128, self.num_anchors *4, 3, padding=1)\n",
    "        \n",
    "        self.cls_head2 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head2 = nn.Conv2d(128,self.num_anchors * 4,3, padding=1)\n",
    "        \n",
    "        self.cls_head3 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head3 = nn.Conv2d(128, self.num_anchors * 4,3, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        step1=self.step1(x)\n",
    "        step2=self.step2(step1)\n",
    "\n",
    "        step3=self.step3(step2)\n",
    "        step4 = self.step4(step3)\n",
    "        step5 = self.step5(step4)\n",
    "                \n",
    "        cls1 = self.cls_head1(step3)\n",
    "        bbox1 = self.bbox_head1(step3)\n",
    "        \n",
    "        cls2 = self.cls_head2(step4)\n",
    "        bbox2 = self.bbox_head2(step4)\n",
    "        \n",
    "        cls3 = self.cls_head3(step5)\n",
    "        bbox3 = self.bbox_head3(step5) \n",
    "\n",
    "    \n",
    "        cls1 = cls1.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        cls2 = cls2.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        cls3 = cls3.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        \n",
    "        bbox1 = bbox1.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox2 = bbox2.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox3 = bbox3.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "      \n",
    "        cls_pred = torch.cat([cls1, cls2, cls3], dim=1)\n",
    "        bbox_pred = torch.cat([bbox1, bbox2, bbox3], dim=1)\n",
    "        \n",
    "        return (bbox_pred, cls_pred)\n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pikachu_ds =PikachuDataset(anchor_areas=[30*30, 50*50, 90*90])\n",
    "pikachu_dl = DataLoader(pikachu_ds, batch_size=2, collate_fn=pikachu_ds.collate_func)\n",
    "\n",
    "# for b in pikachu_dl:\n",
    "#     break\n",
    "# import pandas as pd  \n",
    "# b[0].shape\n",
    "# b[1].shape\n",
    "# pd.DataFrame(b[2][0].cpu().numpy())[0].value_counts()\n",
    "# # b[0].shape\n",
    "\n",
    "# for b in pikachu_dl:\n",
    "#     break\n",
    "    \n",
    "# # b[0].shape\n",
    "# pikachu_ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SimpleSSD(1)\n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(num_classes=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# collect = []\n",
    "\n",
    "\n",
    "# for epoch in range(2): \n",
    "#     for i, b in enumerate(pikachu_dl):\n",
    "#         logs = {}\n",
    "#         optimizer.zero_grad()\n",
    "#         image, bounding_boxes, labels = b\n",
    "#         image = image.to(device)\n",
    "#         bounding_boxes = bounding_boxes.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         loc_pred, cls_pred = model(image)\n",
    "#         total_loss = criterion(loc_pred, bounding_boxes, cls_pred, labels)\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         collect.append([total_loss.detach().cpu().numpy()])\n",
    "#         logs['loss'] = total_loss.item()\n",
    "#         if i % 10 == 0:\n",
    "#             plot_losses.update(logs)\n",
    "#             plot_losses.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-42-e207751f2a84>(69)forward()\n",
      "-> batch_size, num_boxes = cls_targets.size()\n",
      "(Pdb) l\n",
      " 64  \t        loc_preds, cls_preds = pred\n",
      " 65  \t        loc_targets, cls_targets = target\n",
      " 66  \t\n",
      " 67  \t        import pdb; pdb.set_trace()\n",
      " 68  \t\n",
      " 69  ->\t        batch_size, num_boxes = cls_targets.size()\n",
      " 70  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 71  \t        num_pos = pos.data.long().sum()\n",
      " 72  \t\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      "(Pdb) loc_targets\n",
      "tensor([[[ 8.8488,  2.8443,  0.5010, -0.1376],\n",
      "         [ 7.0233,  2.2575,  0.2699, -0.3687],\n",
      "         [ 5.5744,  1.7918,  0.0389, -0.5997],\n",
      "         ...,\n",
      "         [-0.2851, -1.6236, -1.2908, -0.5431],\n",
      "         [-0.2263, -1.2887, -1.5218, -0.7741],\n",
      "         [-0.1796, -1.0228, -1.7529, -1.0052]],\n",
      "\n",
      "        [[ 1.8082,  2.8577,  0.8967,  0.3911],\n",
      "         [ 1.4352,  2.2681,  0.6656,  0.1601],\n",
      "         [ 1.1391,  1.8002,  0.4346, -0.0709],\n",
      "         ...,\n",
      "         [-1.4585, -1.6147, -0.8951, -0.0143],\n",
      "         [-1.1576, -1.2816, -1.1261, -0.2454],\n",
      "         [-0.9188, -1.0172, -1.3572, -0.4764]],\n",
      "\n",
      "        [[ 8.1493,  2.8127,  0.4713,  0.0944],\n",
      "         [ 6.4681,  2.2324,  0.2403, -0.1366],\n",
      "         [ 5.1337,  1.7719,  0.0092, -0.3677],\n",
      "         ...,\n",
      "         [-0.4017, -1.6447, -1.3204, -0.3110],\n",
      "         [-0.3188, -1.3054, -1.5515, -0.5421],\n",
      "         [-0.2531, -1.0361, -1.7825, -0.7731]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.6393,  0.8871,  1.1807,  0.5214],\n",
      "         [ 2.8885,  0.7041,  0.9496,  0.2903],\n",
      "         [ 2.2926,  0.5588,  0.7186,  0.0593],\n",
      "         ...,\n",
      "         [-1.1534, -2.9285, -0.6111,  0.1159],\n",
      "         [-0.9154, -2.3243, -0.8421, -0.1152],\n",
      "         [-0.7266, -1.8448, -1.0732, -0.3462]],\n",
      "\n",
      "        [[ 3.1658,  2.6206,  0.6640,  0.0738],\n",
      "         [ 2.5127,  2.0800,  0.4329, -0.1573],\n",
      "         [ 1.9944,  1.6509,  0.2019, -0.3883],\n",
      "         ...,\n",
      "         [-1.2323, -1.7728, -1.1278, -0.3317],\n",
      "         [-0.9781, -1.4070, -1.3588, -0.5628],\n",
      "         [-0.7763, -1.1168, -1.5899, -0.7938]],\n",
      "\n",
      "        [[ 7.3673,  3.7857,  0.5332,  0.4904],\n",
      "         [ 5.8475,  3.0047,  0.3021,  0.2593],\n",
      "         [ 4.6411,  2.3848,  0.0711,  0.0283],\n",
      "         ...,\n",
      "         [-0.5320, -0.9960, -1.2586,  0.0849],\n",
      "         [-0.4223, -0.7905, -1.4896, -0.1462],\n",
      "         [-0.3352, -0.6275, -1.7207, -0.3772]]], device='cuda:0')\n",
      "(Pdb) loc_targets.shape[0]\n",
      "8\n",
      "(Pdb) loc_targets[0]\n",
      "tensor([[ 8.8488,  2.8443,  0.5010, -0.1376],\n",
      "        [ 7.0233,  2.2575,  0.2699, -0.3687],\n",
      "        [ 5.5744,  1.7918,  0.0389, -0.5997],\n",
      "        ...,\n",
      "        [-0.2851, -1.6236, -1.2908, -0.5431],\n",
      "        [-0.2263, -1.2887, -1.5218, -0.7741],\n",
      "        [-0.1796, -1.0228, -1.7529, -1.0052]], device='cuda:0')\n",
      "(Pdb) l\n",
      " 75  \t        ################################################################\n",
      " 76  \t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  \t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      " 82  \t        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
      " 83  \t        ################################################################\n",
      " 84  \t        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
      " 85  \t        num_peg = pos_neg.data.long().sum()\n",
      "(Pdb) ll\n",
      " 52  \t    def forward(self, pred, target):\n",
      " 53  \t\n",
      " 54  \t        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
      " 55  \t        Args:\n",
      " 56  \t          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
      " 57  \t          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
      " 58  \t          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
      " 59  \t          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
      " 60  \t        loss:\n",
      " 61  \t          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
      " 62  \t        '''\n",
      " 63  \t\n",
      " 64  \t        loc_preds, cls_preds = pred\n",
      " 65  \t        loc_targets, cls_targets = target\n",
      " 66  \t\n",
      " 67  \t        import pdb; pdb.set_trace()\n",
      " 68  \t\n",
      " 69  ->\t        batch_size, num_boxes = cls_targets.size()\n",
      " 70  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 71  \t        num_pos = pos.data.long().sum()\n",
      " 72  \t\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      " 75  \t        ################################################################\n",
      " 76  \t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  \t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      " 82  \t        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
      " 83  \t        ################################################################\n",
      " 84  \t        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
      " 85  \t        num_peg = pos_neg.data.long().sum()\n",
      " 86  \t        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
      " 87  \t        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
      " 88  \t\n",
      " 89  \t#         fl = FL()\n",
      " 90  \t#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
      " 91  \t\n",
      " 92  \t        import pdb\n",
      " 93  \t        pdb.set_trace()\n",
      " 94  \t        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
      " 95  \t#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
      " 96  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 97  \t        num_pos = pos.data.long().sum()\n",
      " 98  \t        num_pos_neg = pos_neg.data.long().sum()\n",
      " 99  \t\n",
      "100  \t        if num_pos > 0:\n",
      "101  \t            loss = (cls_loss + loc_loss) / num_pos\n",
      "102  \t        elif num_pos_neg > 0:\n",
      "103  \t            loss = cls_loss\n",
      "104  \t        else:\n",
      "105  \t            raise Exception('num_pos_neg == 0')\n",
      "106  \t\n",
      "107  \t        loss = loc_loss + cls_loss\n",
      "108  \t        return loss\n",
      "(Pdb) ll\n",
      " 52  \t    def forward(self, pred, target):\n",
      " 53  \t\n",
      " 54  \t        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
      " 55  \t        Args:\n",
      " 56  \t          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
      " 57  \t          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
      " 58  \t          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
      " 59  \t          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
      " 60  \t        loss:\n",
      " 61  \t          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
      " 62  \t        '''\n",
      " 63  \t\n",
      " 64  \t        loc_preds, cls_preds = pred\n",
      " 65  \t        loc_targets, cls_targets = target\n",
      " 66  \t\n",
      " 67  \t        import pdb; pdb.set_trace()\n",
      " 68  \t\n",
      " 69  ->\t        batch_size, num_boxes = cls_targets.size()\n",
      " 70  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 71  \t        num_pos = pos.data.long().sum()\n",
      " 72  \t\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      " 75  \t        ################################################################\n",
      " 76  \t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  \t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      " 82  \t        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
      " 83  \t        ################################################################\n",
      " 84  \t        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
      " 85  \t        num_peg = pos_neg.data.long().sum()\n",
      " 86  \t        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
      " 87  \t        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
      " 88  \t\n",
      " 89  \t#         fl = FL()\n",
      " 90  \t#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
      " 91  \t\n",
      " 92  \t        import pdb\n",
      " 93  \t        pdb.set_trace()\n",
      " 94  \t        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
      " 95  \t#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
      " 96  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 97  \t        num_pos = pos.data.long().sum()\n",
      " 98  \t        num_pos_neg = pos_neg.data.long().sum()\n",
      " 99  \t\n",
      "100  \t        if num_pos > 0:\n",
      "101  \t            loss = (cls_loss + loc_loss) / num_pos\n",
      "102  \t        elif num_pos_neg > 0:\n",
      "103  \t            loss = cls_loss\n",
      "104  \t        else:\n",
      "105  \t            raise Exception('num_pos_neg == 0')\n",
      "106  \t\n",
      "107  \t        loss = loc_loss + cls_loss\n",
      "108  \t        return loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) loc_preds\n",
      "tensor([[[-0.0456, -0.1506,  0.2617, -0.5339],\n",
      "         [-0.0064,  0.1060, -0.1955, -0.0417],\n",
      "         [ 0.2040,  0.3076, -0.0858, -0.1872],\n",
      "         ...,\n",
      "         [-0.0357, -0.2670,  0.0439,  0.0329],\n",
      "         [-0.0967, -0.0485, -0.0843, -0.0971],\n",
      "         [-0.0953,  0.0049,  0.2119,  0.1706]],\n",
      "\n",
      "        [[ 0.2547, -0.4716,  0.3697, -0.4373],\n",
      "         [ 0.0782,  0.0124, -0.1051, -0.0122],\n",
      "         [-0.0499,  0.1857, -0.0480, -0.1715],\n",
      "         ...,\n",
      "         [ 0.0014, -0.1792,  0.0294, -0.1873],\n",
      "         [-0.1598, -0.3353,  0.0073, -0.1370],\n",
      "         [-0.2038,  0.0189,  0.3149,  0.0304]],\n",
      "\n",
      "        [[ 0.1455, -0.3014,  0.2963, -0.6725],\n",
      "         [ 0.0826, -0.0351, -0.5331, -0.2374],\n",
      "         [ 0.3603,  0.0526,  0.1720, -0.1059],\n",
      "         ...,\n",
      "         [ 0.1299, -0.2406,  0.1025, -0.0623],\n",
      "         [-0.3832, -0.1602, -0.4046, -0.2097],\n",
      "         [-0.1315,  0.2797,  0.3770,  0.0726]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1398, -0.2267,  0.3478, -0.3984],\n",
      "         [-0.1103,  0.1563, -0.3587, -0.0804],\n",
      "         [ 0.2971,  0.1476, -0.0307, -0.1783],\n",
      "         ...,\n",
      "         [ 0.1088, -0.1412,  0.0092, -0.2120],\n",
      "         [-0.1843, -0.0560, -0.1794, -0.1184],\n",
      "         [-0.0112,  0.1719,  0.2665,  0.1547]],\n",
      "\n",
      "        [[ 0.2213, -0.3961,  0.2324, -0.3826],\n",
      "         [ 0.1538,  0.0276, -0.2503,  0.0568],\n",
      "         [ 0.3371, -0.0105, -0.0538, -0.1295],\n",
      "         ...,\n",
      "         [-0.0074, -0.5022,  0.0460, -0.0696],\n",
      "         [-0.4438, -0.2010,  0.0635,  0.3253],\n",
      "         [-0.1285,  0.3278,  0.8304, -0.1824]],\n",
      "\n",
      "        [[ 0.0479, -0.1160,  0.2127, -0.3078],\n",
      "         [-0.0372,  0.0105, -0.2305, -0.2331],\n",
      "         [ 0.1993,  0.3109, -0.0196, -0.0730],\n",
      "         ...,\n",
      "         [-0.1398, -0.1538, -0.0307,  0.1540],\n",
      "         [-0.3927, -0.0950, -0.3117,  0.0308],\n",
      "         [-0.1876,  0.1372,  0.5844,  0.1222]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>)\n",
      "(Pdb) loc_preds.shape\n",
      "torch.Size([8, 12096, 4])\n",
      "(Pdb) loc_targets.shape\n",
      "torch.Size([8, 12096, 4])\n",
      "(Pdb) ll\n",
      " 52  \t    def forward(self, pred, target):\n",
      " 53  \t\n",
      " 54  \t        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
      " 55  \t        Args:\n",
      " 56  \t          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
      " 57  \t          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
      " 58  \t          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
      " 59  \t          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
      " 60  \t        loss:\n",
      " 61  \t          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
      " 62  \t        '''\n",
      " 63  \t\n",
      " 64  \t        loc_preds, cls_preds = pred\n",
      " 65  \t        loc_targets, cls_targets = target\n",
      " 66  \t\n",
      " 67  \t        import pdb; pdb.set_trace()\n",
      " 68  \t\n",
      " 69  ->\t        batch_size, num_boxes = cls_targets.size()\n",
      " 70  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 71  \t        num_pos = pos.data.long().sum()\n",
      " 72  \t\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      " 75  \t        ################################################################\n",
      " 76  \t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  \t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      " 82  \t        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
      " 83  \t        ################################################################\n",
      " 84  \t        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
      " 85  \t        num_peg = pos_neg.data.long().sum()\n",
      " 86  \t        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
      " 87  \t        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
      " 88  \t\n",
      " 89  \t#         fl = FL()\n",
      " 90  \t#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
      " 91  \t\n",
      " 92  \t        import pdb\n",
      " 93  \t        pdb.set_trace()\n",
      " 94  \t        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
      " 95  \t#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
      " 96  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 97  \t        num_pos = pos.data.long().sum()\n",
      " 98  \t        num_pos_neg = pos_neg.data.long().sum()\n",
      " 99  \t\n",
      "100  \t        if num_pos > 0:\n",
      "101  \t            loss = (cls_loss + loc_loss) / num_pos\n",
      "102  \t        elif num_pos_neg > 0:\n",
      "103  \t            loss = cls_loss\n",
      "104  \t        else:\n",
      "105  \t            raise Exception('num_pos_neg == 0')\n",
      "106  \t\n",
      "107  \t        loss = loc_loss + cls_loss\n",
      "108  \t        return loss\n",
      "(Pdb) n\n",
      "> <ipython-input-42-e207751f2a84>(70)forward()\n",
      "-> pos = cls_targets > 0  # [N,#anchors]\n",
      "(Pdb) n\n",
      "> <ipython-input-42-e207751f2a84>(71)forward()\n",
      "-> num_pos = pos.data.long().sum()\n",
      "(Pdb) n\n",
      "> <ipython-input-42-e207751f2a84>(76)forward()\n",
      "-> mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      "(Pdb) ll\n",
      " 52  \t    def forward(self, pred, target):\n",
      " 53  \t\n",
      " 54  \t        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
      " 55  \t        Args:\n",
      " 56  \t          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
      " 57  \t          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
      " 58  \t          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
      " 59  \t          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
      " 60  \t        loss:\n",
      " 61  \t          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
      " 62  \t        '''\n",
      " 63  \t\n",
      " 64  \t        loc_preds, cls_preds = pred\n",
      " 65  \t        loc_targets, cls_targets = target\n",
      " 66  \t\n",
      " 67  \t        import pdb; pdb.set_trace()\n",
      " 68  \t\n",
      " 69  \t        batch_size, num_boxes = cls_targets.size()\n",
      " 70  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 71  \t        num_pos = pos.data.long().sum()\n",
      " 72  \t\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      " 75  \t        ################################################################\n",
      " 76  ->\t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  \t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      " 82  \t        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
      " 83  \t        ################################################################\n",
      " 84  \t        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
      " 85  \t        num_peg = pos_neg.data.long().sum()\n",
      " 86  \t        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
      " 87  \t        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
      " 88  \t\n",
      " 89  \t#         fl = FL()\n",
      " 90  \t#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
      " 91  \t\n",
      " 92  \t        import pdb\n",
      " 93  \t        pdb.set_trace()\n",
      " 94  \t        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
      " 95  \t#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
      " 96  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 97  \t        num_pos = pos.data.long().sum()\n",
      " 98  \t        num_pos_neg = pos_neg.data.long().sum()\n",
      " 99  \t\n",
      "100  \t        if num_pos > 0:\n",
      "101  \t            loss = (cls_loss + loc_loss) / num_pos\n",
      "102  \t        elif num_pos_neg > 0:\n",
      "103  \t            loss = cls_loss\n",
      "104  \t        else:\n",
      "105  \t            raise Exception('num_pos_neg == 0')\n",
      "106  \t\n",
      "107  \t        loss = loc_loss + cls_loss\n",
      "108  \t        return loss\n",
      "(Pdb) pos.shape\n",
      "torch.Size([8, 12096])\n",
      "(Pdb) pos\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) cls_targets.shape\n",
      "torch.Size([8, 12096])\n",
      "(Pdb) cls_targets[0]\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "(Pdb) pos.shape\n",
      "torch.Size([8, 12096])\n",
      "(Pdb) pos[0]\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)\n",
      "(Pdb) l\n",
      " 71  \t        num_pos = pos.data.long().sum()\n",
      " 72  \t\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      " 75  \t        ################################################################\n",
      " 76  ->\t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  \t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      "(Pdb) loc_preds.shape\n",
      "torch.Size([8, 12096, 4])\n",
      "(Pdb) n\n",
      "> <ipython-input-42-e207751f2a84>(77)forward()\n",
      "-> masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      "(Pdb) mask.shape\n",
      "torch.Size([8, 12096, 4])\n",
      "(Pdb) mask.shape\n",
      "torch.Size([8, 12096, 4])\n",
      "(Pdb) mask[0]\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)\n",
      "(Pdb) n\n",
      "> <ipython-input-42-e207751f2a84>(78)forward()\n",
      "-> masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      "(Pdb) l\n",
      " 73  \t        ################################################################\n",
      " 74  \t        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
      " 75  \t        ################################################################\n",
      " 76  \t        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
      " 77  \t        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
      " 78  ->\t        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
      " 79  \t        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
      " 80  \t\n",
      " 81  \t        ################################################################\n",
      " 82  \t        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
      " 83  \t        ################################################################\n",
      "(Pdb) masked_loc_preds.shape\n",
      "torch.Size([273, 4])\n",
      "(Pdb) mask.shape\n",
      "torch.Size([8, 12096, 4])\n",
      "(Pdb) mask[0]\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)\n",
      "(Pdb) loc_preds[mask]\n",
      "tensor([ 0.5231,  0.2751,  0.4869,  ...,  0.3332, -1.1037,  0.3961],\n",
      "       device='cuda:0', grad_fn=<IndexBackward>)\n",
      "(Pdb) loc_preds[mask]\n",
      "tensor([ 0.5231,  0.2751,  0.4869,  ...,  0.3332, -1.1037,  0.3961],\n",
      "       device='cuda:0', grad_fn=<IndexBackward>)\n",
      "(Pdb) mask\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]], device='cuda:0', dtype=torch.uint8)\n",
      "(Pdb) mask.sum(1)\n",
      "tensor([[36, 36, 36, 36],\n",
      "        [39, 39, 39, 39],\n",
      "        [35, 35, 35, 35],\n",
      "        [36, 36, 36, 36],\n",
      "        [29, 29, 29, 29],\n",
      "        [25, 25, 25, 25],\n",
      "        [42, 42, 42, 42],\n",
      "        [31, 31, 31, 31]], device='cuda:0')\n",
      "(Pdb) mask.max(0)\n",
      "(tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8), tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], device='cuda:0'))\n",
      "(Pdb) l\n",
      " 84  \t        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
      " 85  \t        num_peg = pos_neg.data.long().sum()\n",
      " 86  \t        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
      " 87  \t        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
      " 88  \t\n",
      " 89  \t#         fl = FL()\n",
      " 90  \t#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
      " 91  \t\n",
      " 92  \t        import pdb\n",
      " 93  \t        pdb.set_trace()\n",
      " 94  \t        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
      "(Pdb) pos.shape\n",
      "torch.Size([8, 12096])\n",
      "(Pdb) pos.sum(1)\n",
      "tensor([36, 39, 35, 36, 29, 25, 42, 31], device='cuda:0')\n",
      "(Pdb) l\n",
      " 95  \t#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
      " 96  \t        pos = cls_targets > 0  # [N,#anchors]\n",
      " 97  \t        num_pos = pos.data.long().sum()\n",
      " 98  \t        num_pos_neg = pos_neg.data.long().sum()\n",
      " 99  \t\n",
      "100  \t        if num_pos > 0:\n",
      "101  \t            loss = (cls_loss + loc_loss) / num_pos\n",
      "102  \t        elif num_pos_neg > 0:\n",
      "103  \t            loss = cls_loss\n",
      "104  \t        else:\n",
      "105  \t            raise Exception('num_pos_neg == 0')\n",
      "(Pdb) pos.sum()\n",
      "tensor(273, device='cuda:0')\n",
      "(Pdb) cls_targets.shape\n",
      "torch.Size([8, 12096])\n",
      "(Pdb) cls_targets[0]\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-0e94678c92a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 batch_size=8, device='cuda')\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpikachu_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_train_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0myi_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my_train_is_ph\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myi_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, Xi, yi, **fit_params)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mstep_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_step_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m             \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mtrain_step_single\u001b[0;34m(self, Xi, yi, **fit_params)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(self, y_pred, y_true, X, training)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \"\"\"\n\u001b[1;32m   1070\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-e207751f2a84>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_preds\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# [N,#anchors,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmasked_loc_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# [#pos,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mmasked_loc_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [#pos,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mloc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_loc_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_loc_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-e207751f2a84>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_preds\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# [N,#anchors,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmasked_loc_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# [#pos,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mmasked_loc_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [#pos,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mloc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_loc_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_loc_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skorch.net import NeuralNet\n",
    "\n",
    "net = NeuralNet(model,\n",
    "                criterion=FocalLoss, \n",
    "                criterion__num_classes=1,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                lr=0.0001,\n",
    "                batch_size=8, device='cuda')\n",
    "\n",
    "net.fit(pikachu_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = net.forward(pikachu_ds)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dc9d24a1c5db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToPILImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpikachu_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-577a68198978>\u001b[0m in \u001b[0;36mcollate_func\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-577a68198978>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize, ToPILImage, ToTensor\n",
    "it = iter(pikachu_dl)\n",
    "b = next(it)\n",
    "\n",
    "im = ToTensor()(Resize((256, 256))(ToPILImage()(torch.cat([b[0][0], b[0][0]], dim=1))))\n",
    "image = im.unsqueeze(0)\n",
    "\n",
    "enc = DataEncoder(anchor_areas=[30*30, 50*50, 90*90])\n",
    "loc_pred, cls_pred = model(image.cuda())\n",
    "\n",
    "# i = 0\n",
    "# bbspred, labelpred, score  = enc.decode(\n",
    "#     loc_pred[i].float().cpu(), \n",
    "#     cls_pred[i].float().cpu(), \n",
    "#     torch.Tensor([256, 256]).float().cpu()\n",
    "# )\n",
    "\n",
    "# image_to_show = np.moveaxis(\n",
    "#     image[i].detach().cpu().numpy(),0, 2)\n",
    "\n",
    "# matched_anchors_on_image = ia.BoundingBoxesOnImage(\n",
    "#     [ia.BoundingBox(*b) for b in bbspred.detach().cpu().numpy()], shape=(256, 256))\n",
    "\n",
    "# image_to_show = matched_anchors_on_image.draw_on_image(image_to_show, thickness=2)\n",
    "# plt.imshow(image_to_show)\n",
    "# plt.title('score ' + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Load a custom image with pikachu( or many ) and try to make predictiosn with the network and visualize the result\n",
    "-  Can you think of anything that could confuse our detector? yellow dots ?\n",
    "- Currently the code is not really modular, try to make it nice by splitting it into logical parts\n",
    "    - Base feature extractor Module\n",
    "    - Head Creator module\n",
    "- Currently the detection/cls HEADS are very simple (just one CONV layer) they can be more complex. Try using more convolutions, check other architectures how its done\n",
    "\n",
    "- Can you use our network to train using some new data for instance:\n",
    "    - https://www.kaggle.com/tomluther/ships-in-google-earth\n",
    "    - https://www.kaggle.com/aruchomu/data-for-yolo-v3-kernel\n",
    "    - https://www.kaggle.com/dataturks/face-detection-in-images\n",
    "    - https://www.kaggle.com/dataturks/face-dataset-with-age-emotion-ethnicity  \n",
    " You will need to create a data loader/data sets similar as we did for the pikachu loader. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_sigmoid_focal_loss(pred,\n",
    "                          target,\n",
    "                          weight,\n",
    "                          gamma=2.0,\n",
    "                          alpha=0.25,\n",
    "                          reduction='mean'):\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    target = target.type_as(pred)\n",
    "    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "    weight = (alpha * target + (1 - alpha) * (1 - target)) * weight\n",
    "    weight = weight * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, target, reduction='none') * weight\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()\n",
    "    \n",
    "    \n",
    "def focal_loss(x, y, num_class=2):\n",
    "    '''Focal loss.\n",
    "    Args:\n",
    "      x: (tensor) sized [N,D].\n",
    "      y: (tensor) sized [N,].\n",
    "    Return:\n",
    "      (tensor) focal loss.\n",
    "    '''\n",
    "    alpha = 0.25\n",
    "    gamma = 2\n",
    "\n",
    "    t = one_hot_embedding(y.data.cpu(), num_class)  # [N,21]\n",
    "#     t = t[:,1:]  # exclude background\n",
    "#     t = Variable(t).cuda()  # [N,20]\n",
    "\n",
    "\n",
    "    p = x.sigmoid().detach()\n",
    "    pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
    "    w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
    "    w = w * (1-pt).pow(gamma)\n",
    "    return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1840)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([0, 0.1, 0, 1.0])\n",
    "target = torch.tensor([0,1,1,0])\n",
    "\n",
    "py_sigmoid_focal_loss(pred, target, weight=torch.tensor([1.0,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0433)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([10, 2])\n",
    "y = torch.rand([10, 2]).round().int().float()\n",
    "x\n",
    "\n",
    "x = torch.tensor([[10000.0, 0.0, 10]])\n",
    "y = torch.tensor([[1, 0.0, 1]])\n",
    "\n",
    "py_sigmoid_focal_loss(x, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.stack([torch.ones_like(pred) - pred, pred]).view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-99ef6afa34d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-221-ca8ef2645337>\u001b[0m in \u001b[0;36mfocal_loss\u001b[0;34m(x, y, n)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# pt = p if t > 0 else 1-p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# w = alpha if t > 0 else 1-alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "target = target.reshape(-1, 1)\n",
    "focal_loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([1])\n",
      "> <ipython-input-254-ff27f5bc0c68>(41)focal_loss()\n",
      "-> p = x.sigmoid().detach()\n",
      "(Pdb) l\n",
      " 36  \t    t = t[:,1:]  # exclude background\n",
      " 37  \t#     t = Variable(t).cuda()  # [N,20]\n",
      " 38  \t    import pdb\n",
      " 39  \t    pdb.set_trace()\n",
      " 40  \t\n",
      " 41  ->\t    p = x.sigmoid().detach()\n",
      " 42  \t    pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
      " 43  \t    w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
      " 44  \t    w = w * (1-pt).pow(gamma)\n",
      " 45  \t    return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "[EOF]\n",
      "(Pdb) x.shape\n",
      "torch.Size([1, 3])\n",
      "(Pdb) y.shape\n",
      "torch.Size([1])\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(42)focal_loss()\n",
      "-> pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(43)focal_loss()\n",
      "-> w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(44)focal_loss()\n",
      "-> w = w * (1-pt).pow(gamma)\n",
      "(Pdb) n\n",
      "> <ipython-input-254-ff27f5bc0c68>(45)focal_loss()\n",
      "-> return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "(Pdb) n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))\n",
      "> <ipython-input-254-ff27f5bc0c68>(45)focal_loss()\n",
      "-> return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-254-ff27f5bc0c68>(45)focal_loss()->None\n",
      "-> return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
      "(Pdb) n\n",
      "ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))\n",
      "> <ipython-input-255-fb104846a17c>(5)<module>()\n",
      "-> focal_loss(x, y, n=3)\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-255-fb104846a17c>(5)<module>()->None\n",
      "-> focal_loss(x, y, n=3)\n",
      "(Pdb) n\n",
      "ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))\n",
      "> /home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2961)run_code()\n",
      "-> exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "(Pdb) n\n",
      "> /home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2964)run_code()\n",
      "-> sys.excepthook = old_excepthook\n",
      "(Pdb) n\n",
      "> /home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py(2965)run_code()\n",
      "-> except SystemExit as e:\n",
      "(Pdb) c\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-fb104846a17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-254-ff27f5bc0c68>\u001b[0m in \u001b[0;36mfocal_loss\u001b[0;34m(x, y, n)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# w = alpha if t > 0 else 1-alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 3]))"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Pdb) masked_cls_preds.shape\n",
    "torch.Size([96382, 1])\n",
    "(Pdb) cls_targets[pos_neg].shape\n",
    "torch.Size([96382])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.9, 0, 0]])\n",
    "y = torch.tensor([0])\n",
    "\n",
    "\n",
    "# t = one_hot_embedding(torch.tensor(y), 3) \n",
    "# t = one_hot_embedding(y, 1+1) # [N,21]\n",
    "# print(t)\n",
    "# t = t[:,1:]  # exclude background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-272-2febcda2fe0e>(41)focal_loss()\n",
      "-> p = x.sigmoid().detach()\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2671)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(x, y, num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def box_iou(box1, box2, order='xyxy'):\n",
    "    '''Compute the intersection over union of two set of boxes.\n",
    "\n",
    "    The default box order is (xmin, ymin, xmax, ymax).\n",
    "\n",
    "    Args:\n",
    "      box1: (tensor) bounding boxes, sized [N,4].\n",
    "      box2: (tensor) bounding boxes, sized [M,4].\n",
    "      order: (str) box order, either 'xyxy' or 'xywh'.\n",
    "\n",
    "    Return:\n",
    "      (tensor) iou, sized [N,M].\n",
    "\n",
    "    Reference:\n",
    "      https://github.com/chainer/chainercv/blob/master/chainercv/utils/bbox/bbox_iou.py\n",
    "    '''\n",
    "    if order == 'xywh':\n",
    "        box1 = change_box_order(box1, 'xywh2xyxy')\n",
    "        box2 = change_box_order(box2, 'xywh2xyxy')\n",
    "\n",
    "    N = box1.size(0)\n",
    "    M = box2.size(0)\n",
    "\n",
    "    lt = torch.max(box1[:,None,:2], box2[:,:2])  # [N,M,2]\n",
    "    rb = torch.min(box1[:,None,2:], box2[:,2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb-lt+1).clamp(min=0)      # [N,M,2]\n",
    "    inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n",
    "\n",
    "    area1 = (box1[:,2]-box1[:,0]+1) * (box1[:,3]-box1[:,1]+1)  # [N,]\n",
    "    area2 = (box2[:,2]-box2[:,0]+1) * (box2[:,3]-box2[:,1]+1)  # [M,]\n",
    "    iou = inter / (area1[:,None] + area2 - inter)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = torch.tensor([[10.0,10.0,20,20],[8,8,15,15], [3,3,10,10], [9,9,20,20], [30,30,40,40]])\n",
    "b2 = torch.tensor([[10.0, 10,23, 20],[12,12,30,30], [28,28,35, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = box_iou(b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7857, 0.2020, 0.0000],\n",
       "        [0.1978, 0.0391, 0.0000],\n",
       "        [0.0046, 0.0000, 0.0000],\n",
       "        [0.6836, 0.1910, 0.0000],\n",
       "        [0.0000, 0.0021, 0.2416]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, ix = ious.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 23., 20.],\n",
       "        [10., 10., 23., 20.],\n",
       "        [10., 10., 23., 20.],\n",
       "        [10., 10., 23., 20.],\n",
       "        [28., 28., 35., 35.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 20., 20.],\n",
       "        [10., 10., 20., 20.],\n",
       "        [10., 10., 20., 20.],\n",
       "        [10., 10., 20., 20.],\n",
       "        [ 3.,  3., 10., 10.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
