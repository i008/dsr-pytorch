{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Shot Multi Scale Object Detection Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "from easyimages.utils import change_box_order, vis_image\n",
    "from livelossplot import PlotLosses\n",
    "from pycocotools.coco import COCO\n",
    "from retinanet.encoder import DataEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from retinanet.utils import one_hot_embedding\n",
    "from retinanet.loss import FocalLoss\n",
    "\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 10]\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Object detection is the process of localizing and classyfing a region of interest in an image. Usually by prediting the coordinates of a bounding box enclosing this object.\n",
    "\n",
    "In this notebook we focus on single shot methods detection which means that the detection happens during a single forward pass. There is no second network learning object proposals which would be the case in RCNN architectures.\n",
    "\n",
    "\n",
    "\n",
    "# Example (Synthetic) Object Detection Dataset\n",
    "\n",
    "Our problem boils down to predicting the coordinates and label of the red box given an input image.\n",
    "\n",
    "![alt text](static/shapes-coco.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataset Annotations Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCO format  is the standard for storing object detection and segmentation data so we will work on an example  following the same.\n",
    "\n",
    "In COCO we follow the **xywh** convention or as i like to call it *tlwh*: **(top-left-width-height)**\n",
    "that way you can not confuse it with for instance *cwh*: **(center-point, w, h)**\n",
    "\n",
    "![alt text](static/cocodsformat.jpg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box Encoding\n",
    "\n",
    "Probably the most challenging concept to grasp while learning  object detection is the way how we encode our ground truth information. This is a crucial step in most applications in  deep learning, usually its not that complicated for instance encoding categories as one-hot encoded vectors is a straight forward task, but it can get quite challenging for example in NLP problems where its common to have variable sequence lenghts.\n",
    "\n",
    "In a regular bounding box setup your image can have different number of boxes and classes per image. So how do you encode this information into a fixed size target-tensor?\n",
    "\n",
    "The solution to this problem are **Anchor Boxes** also known as prior boxes.\n",
    "\n",
    "Essentialy Anchor Boxes are a finite set of boundig boxes with different sizes and aspect ratios for each position in the feature map. Usually there are thousands of anchor boxes in modern OD architectures - but only few will \"match\" with our ground truth.\n",
    "\n",
    "\n",
    "![alt text](static/anchors3.png \"All\")\n",
    "\n",
    "Here we visualize 100 random anchor boxes. Next up we will find which ones match our ground-truth using a crieteria known as IoU (Intersection over union)\n",
    "\n",
    "![alt text](static/iou.png \"All\")\n",
    "\n",
    "\n",
    "![alt text](static/matched_anchors.png \"All\")\n",
    "\n",
    "\n",
    "# Bounding Box Regression Coefficients\n",
    "\n",
    "Since predicting the anchor itself would be suboptimal (having IoU's close to 1.0 is unlikely even with a very high number of anchors) Thats why we want our network to predict an offset between the real box (T) and the Anchor (O) and for the width and height we learn the ratio between the Target width and Anchor width. Furthermore its quite common to log-transform this target for more efficient training.\n",
    "\n",
    "We will only use coefficients coming from Anchor boxes with high IoU (usually above 0.5) the rest is ignored in regression loss computation.\n",
    "\n",
    "![alt text](static/anchor_coef.png \"All\")\n",
    "\n",
    "Now the question arises how do you predict those offsets, the trick to this is attaching the detection heads to regular feature maps of a convolutiona neural network (base network)\n",
    "\n",
    "To understand what it means here is a qucik recap of a typical CNN architecture w.r.t Object Detection an important concept is the receptive field of a neuron and the fact that the deeper you go in regular CNN you trade resolution for \"broad-knowledge\" neurons deep in a neural network are more context aware.\n",
    "\n",
    "![alt text](static/cnn.jpg \"All\")\n",
    "\n",
    "Given that it would make sense to use higher resolution feature maps (for instance the 56x56 f-map) to predict small objects and lower resolution ones for big objects. And thats exactly the idea behind **multi scale object detection**\n",
    "\n",
    "\n",
    "\n",
    "# Regression and Detection Heads\n",
    "\n",
    "A regression and detection head is nothing else then a Conv2D block that is convolving a particular feature map. In the most simplified scennario it would work like this:\n",
    "\n",
    "```python\n",
    "num_classes = 3\n",
    "fmap1 = torch.random(BS, 256, 9, 9) # Lets assume this is a 9x9 feature map with depth=256.\n",
    "bbox_regression_head = nn.Conv2d(32, n_anchors * 4, 3, padding=1)\n",
    "bbox_classification_head = nn.Conv2d(32, n_anchors * num_classes, 3, padding=1)\n",
    "\n",
    "bbox_offsets = bbox_regression_head(fmap1)\n",
    "bbox_cls_pred = bbox_classification_head(fmap1)\n",
    "\n",
    "```\n",
    "n_anchors is the number of anchor per location (its pretty common this number is 9 as we have 3 scale ratios and 3 aspect ratios) so the product of those is 9.\n",
    "For regression we multiple this by **4** (we need 4 floats for each anchor to predict the offsets tx,ty,tw,th)\n",
    "\n",
    "Notice how elegant this solution actually is - we have a fully convolutional architecture (which is usually quite efficient memory and speed-wise) it also makes sense intuitivly - convolution is nothing else but sliding a filter on each position of the feature maps, and we know that those neurons have 'knowledge' about a particular part of the image (among other things)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Encoding steps put together\n",
    "\n",
    "To recap the above:  here are the steps neccesery to encode our target bounding boxes:\n",
    "\n",
    "- For each feature map that we want to use for predictions generate a set of anchor-boxes with different size and aspect ratios\n",
    "- Calculate the IoU between your target boxes and anchor boxes\n",
    "- For anchors that had the biggest IoU  calculate the offset values.\n",
    "    tx = (x - anchor_x) / anchor_w\n",
    "    ty = (y - anchor_y) / anchor_h\n",
    "    tw = log(w / anchor_w)\n",
    "    th = log(h / anchor_h)\n",
    "\n",
    "- For box regression only offsets  that matched with an IoU > 0.5 will be used during loss computation\n",
    "- For box classification only boxes with IoU > 0.5 and  IoU < 0.4 will be used during loss computetion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.6667,  16.6667, 141.4214, 282.8427],\n",
       "        [ 16.6667,  16.6667, 176.7767, 353.5534],\n",
       "        [ 16.6667,  16.6667, 212.1320, 424.2641],\n",
       "        ...,\n",
       "        [283.3333, 283.3333, 282.8427, 141.4214],\n",
       "        [283.3333, 283.3333, 353.5534, 176.7767],\n",
       "        [283.3333, 283.3333, 424.2641, 212.1320]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "de = DataEncoder(anchor_areas=[50*50, 100*100, 200*200],\n",
    "                 aspect_ratios=[0.5, 1, 2],\n",
    "                 scale_ratios=[1.0, 1.25, 1.5],\n",
    "                 fm_sizes=torch.tensor([(37, 37),\n",
    "                                        (18, 18),\n",
    "                                        (9, 9)]).float(),\n",
    "\n",
    "                 min_iou=0.5\n",
    "                 )\n",
    "\n",
    "anchors, anchors_per_fm = de._get_anchor_boxes(torch.Tensor((300, 300)))\n",
    "\n",
    "# for fms in anchors_per_fm:\n",
    "#     print(fms.shape)\n",
    "    \n",
    "# fms\n",
    "\n",
    "anchors_per_fm[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure we understand where the number are coming from:\n",
    "\n",
    "- We have 12321 anchors on fm1 (37 * 37 *9)\n",
    "- We have 2916 anchors on fm2 (18 * 18 * 9)\n",
    "- We have 729 anchors on fm3 (9 * 9 * 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping COCO dataset into a pytorch dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annotation_file='generate-shapes/coco_shapes_100_200/100_200.json')\n",
    "\n",
    "\n",
    "class CoCoDS():\n",
    "    def __init__(self, coco, base_path, encoder):\n",
    "        self.coco = coco\n",
    "        self.base_path = base_path\n",
    "        self.encoder = encoder\n",
    "        self.all_images = coco.getImgIds()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        image_id = self.all_images[i]\n",
    "        image_meta = coco.loadImgs(ids=[image_id])[0]\n",
    "        image = PIL.Image.open(os.path.join(self.base_path, image_id))\n",
    "        image = ToTensor()(image)\n",
    "\n",
    "        w, h = image_meta['width'], image_meta['height']\n",
    "        annot = coco.loadAnns(ids=coco.getAnnIds(imgIds=[image_id]))\n",
    "        boxes = torch.tensor([a['bbox'] for a in annot]).float()\n",
    "        labels = torch.tensor([a['category_id']for a in annot]).float()\n",
    "        boxes = change_box_order(\n",
    "            boxes, input_order='tlwh', output_order='tlbr')\n",
    "\n",
    "        loc_targets, cls_targets = self.encoder.encode(boxes, labels, (w, h))\n",
    "\n",
    "        return image, loc_targets, cls_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15966, 4])\n",
      "torch.Size([15966])\n"
     ]
    }
   ],
   "source": [
    "cocds = CoCoDS(coco, 'generate-shapes/coco_shapes_100_200/images', de)\n",
    "\n",
    "_, loc, cls = cocds[0]\n",
    "\n",
    "print(loc.shape)\n",
    "print(cls.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets model a very simple network\n",
    "Our network is really simple it consists of 5 downsample steps. Each downsample is a **Conv2->BN->Relu->Conv2->BN->Relu->MaxPool2D** block. \n",
    "\n",
    "We will attach our detection heads to f-map3 f-map4 and f-map5. Lets caculate the shapes of those feature maps:\n",
    "\n",
    "outpu shape  after step 3 -> 300 / 2 / 2 / 2 = 37   \n",
    "output shape after step 4 -> 300 / 2 / 2 / 2 / 2  = 18   \n",
    "output shape after step 5 -> 300 / 2 / 2 / 2 / 2 / 2 = 9   \n",
    "\n",
    "As you can imagine the Anchor boxes reflect this architecture. In the end all the shape must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleSSD(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cls=1, num_anchors=9):\n",
    "        super(SimpleSSD, self).__init__()\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        # Base CNN (think resnet/vgg or other base network)\n",
    "        self.step1 = self.down_sample(3, 128)\n",
    "        self.step2 = self.down_sample(128, 128)\n",
    "        self.step3 = self.down_sample(128, 128)\n",
    "        self.step4 = self.down_sample(128, 128)\n",
    "        self.step5 = self.down_sample(128, 128)\n",
    "\n",
    "        self.cls_head1 = self.create_cls_head(\n",
    "            128, self.n_cls, self.num_anchors)\n",
    "        self.bbox_head1 = self.create_bbox_head(128, self.num_anchors)\n",
    "\n",
    "        self.cls_head2 = self.create_cls_head(\n",
    "            128, self.n_cls, self.num_anchors)\n",
    "        self.bbox_head2 = self.create_bbox_head(128, self.num_anchors)\n",
    "\n",
    "        self.cls_head3 = self.create_cls_head(\n",
    "            128, self.n_cls, self.num_anchors)\n",
    "        self.bbox_head3 = self.create_bbox_head(128, self.num_anchors)\n",
    "\n",
    "    @staticmethod\n",
    "    def down_sample(in_channels, out_channels):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_cls_head(n_in, n_cls, num_anchors):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(n_in, 92, 3, padding=1),\n",
    "            nn.BatchNorm2d(92),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(92, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, num_anchors * n_cls, 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_bbox_head(n_in, num_anchors):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(n_in, 92, 3, padding=1),\n",
    "            nn.BatchNorm2d(92),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(92, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,  num_anchors * 4, 3, stride=1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x -> [BS, 3, 300, 300]\n",
    "\n",
    "        step1 = self.step1(x)\n",
    "        step2 = self.step2(step1)\n",
    "        step3 = self.step3(step2)\n",
    "        step4 = self.step4(step3)\n",
    "        step5 = self.step5(step4)\n",
    "\n",
    "        cls1 = self.cls_head1(step3)\n",
    "        bbox1 = self.bbox_head1(step3)\n",
    "\n",
    "        cls2 = self.cls_head2(step4)\n",
    "        bbox2 = self.bbox_head2(step4)\n",
    "\n",
    "        cls3 = self.cls_head3(step5)\n",
    "        bbox3 = self.bbox_head3(step5)\n",
    "        \n",
    "        \n",
    "\n",
    "        cls1 = cls1.permute(0, 2, 3, 1).contiguous().view(\n",
    "            x.size(0), -1, self.n_cls)\n",
    "        cls2 = cls2.permute(0, 2, 3, 1).contiguous().view(\n",
    "            x.size(0), -1, self.n_cls)\n",
    "        cls3 = cls3.permute(0, 2, 3, 1).contiguous().view(\n",
    "            x.size(0), -1, self.n_cls)\n",
    "\n",
    "        bbox1 = bbox1.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox2 = bbox2.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox3 = bbox3.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
    "\n",
    "        cls_pred = torch.cat([cls1, cls2, cls3], dim=1)\n",
    "        bbox_pred = torch.cat([bbox1, bbox2, bbox3], dim=1)\n",
    "\n",
    "        return bbox_pred, cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\tloss             \t (min:    6.705, max: 29366.072, cur:   67.510)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "de = DataEncoder(anchor_areas=[50*50, 100*100, 200*200],\n",
    "                 aspect_ratios=[0.5, 1, 2],\n",
    "                 scale_ratios=[1.0, 1.25, 1.5],\n",
    "                 fm_sizes=torch.tensor([(37, 37),\n",
    "                                        (18, 18),\n",
    "                                        (9, 9)]).float(),\n",
    "\n",
    "                 min_iou=0.5\n",
    "                 )\n",
    "\n",
    "coco = COCO(annotation_file='generate-shapes/coco_shapes_50_100/50_100.json')\n",
    "coco_ds = CoCoDS(coco, 'generate-shapes/coco_shapes_50_100/images', de)\n",
    "coco_dl = DataLoader(coco_ds, batch_size=4, sampler=RandomSampler(coco_ds))\n",
    "\n",
    "model = SimpleSSD(n_cls=3)\n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(num_classes=3)\n",
    "plot_losses = PlotLosses()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "collect = []\n",
    "for epoch in range(10):\n",
    "    for i, b in enumerate(coco_dl):\n",
    "        print(i)\n",
    "        logs = {}\n",
    "        optimizer.zero_grad()\n",
    "        image, bounding_boxes, labels = b\n",
    "        image = image.to(device)\n",
    "        bounding_boxes = bounding_boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        loc_pred, cls_pred = model(image)\n",
    "        total_loss = criterion(loc_pred, bounding_boxes, cls_pred, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        collect.append([total_loss.detach().cpu().numpy()])\n",
    "        logs['loss'] = total_loss.item()\n",
    "        if i % 1 == 0:\n",
    "            plot_losses.update(logs)\n",
    "            plot_losses.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15966, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls_pred.shape\n",
    "it = iter(coco_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image, _, _ = next(it)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loc_pred, cls_pred = model(image.cuda())\n",
    "\n",
    "i = 1\n",
    "bbspred, labelpred, score = de.decode(\n",
    "    loc_pred[i].float().cpu(),\n",
    "    cls_pred[i].float().cpu(),\n",
    "    torch.Tensor([300, 300]).float().cpu(),\n",
    "    cls_thresh=0.2, nms_thresh=0.01\n",
    ")\n",
    "imagepil = ToPILImage()(image[1])\n",
    "\n",
    "d = {0: 'circle', 1: 'triangle',  2: 'rectanglee'}\n",
    "B = bbspred.detach().cpu().numpy()\n",
    "L = labelpred.detach().cpu().numpy()\n",
    "S = score.detach().cpu().numpy()\n",
    "\n",
    "L = [d[i] for i in L]\n",
    "f = vis_image(imagepil, bbspred.detach().numpy(), label_names=L, scores=S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "uuid": "d29f5467-c5cd-48fb-83bf-d890816a35d0"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
