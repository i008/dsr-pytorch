{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "\n",
    "import imgaug as ia\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from livelossplot import PlotLosses\n",
    "from matplotlib import pyplot as plt\n",
    "from skorch.net import NeuralNet\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, ToPILImage, ToTensor\n",
    "\n",
    "from retinanet.encoder import DataEncoder\n",
    "from retinanet.loss import *\n",
    "from retinanet.loss import FocalLoss\n",
    "from retinanet.retinanet import RetinaNet\n",
    "from utils.pikachu_dataset import load_data_pikachu\n",
    "\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 10]\n",
    "\n",
    "\n",
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def focal_loss(self, x, y):\n",
    "        '''Focal loss.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        alpha = 0.25\n",
    "        gamma = 2\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)  # [N,21]\n",
    "        t = t[:,1:]  # exclude background\n",
    "        t = Variable(t).cuda()  # [N,20]\n",
    "\n",
    "        p = x.sigmoid().detach()\n",
    "        pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
    "        w = alpha*t + (1-alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
    "        w = w * (1-pt).pow(gamma)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
    "\n",
    "    def focal_loss_alt(self, x, y):\n",
    "        '''Focal loss alternative.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        alpha = 0.25\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)\n",
    "        t = t[:,1:]\n",
    "        t = Variable(t).cuda()\n",
    "\n",
    "        xt = x*(2*t-1)  # xt = x if t > 0 else -x\n",
    "        pt = (2*xt+1).sigmoid() \n",
    "\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        loss = -w*pt.log() / 2\n",
    "        return loss.sum()\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
    "          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
    "          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
    "        loss:\n",
    "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets).\n",
    "        '''\n",
    "        batch_size, num_boxes = cls_targets.size()\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.data.long().sum()\n",
    "\n",
    "        ################################################################\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        ################################################################\n",
    "        mask = pos.unsqueeze(2).expand_as(loc_preds)       # [N,#anchors,4]\n",
    "        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos,4]\n",
    "        masked_loc_targets = loc_targets[mask].view(-1,4)  # [#pos,4]\n",
    "        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, size_average=False)\n",
    "\n",
    "        ################################################################\n",
    "        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
    "        ################################################################\n",
    "        pos_neg = cls_targets > -1  # exclude ignored anchors\n",
    "        num_peg = pos_neg.data.long().sum()\n",
    "        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
    "        masked_cls_preds = cls_preds[mask].view(-1,self.num_classes)\n",
    "        \n",
    "#         fl = FL()\n",
    "#         cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
    "        cls_loss = self.focal_loss(masked_cls_preds, cls_targets[pos_neg])\n",
    "#         print(\"clsloss: {} locloss: {}\".format(cls_loss.item(), loc_loss.item()))\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.data.long().sum()\n",
    "        num_pos_neg = pos_neg.data.long().sum()\n",
    "\n",
    "        if num_pos > 0:\n",
    "            loss = (cls_loss + loc_loss) / num_pos\n",
    "        elif num_pos_neg > 0:\n",
    "            loss = cls_loss\n",
    "        else:\n",
    "            raise Exception('num_pos_neg == 0')\n",
    "            \n",
    "        loss = loc_loss + cls_loss\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "class PikachuDataset(Dataset):\n",
    "    def __init__(self, anchor_areas=None):\n",
    "        \n",
    "        self.anchor_areas = anchor_areas\n",
    "        self.train, self.val = load_data_pikachu(1)\n",
    "        if anchor_areas is not None:\n",
    "            encoder = DataEncoder(anchor_areas=anchor_areas)\n",
    "        else:\n",
    "            encoder = DataEncoder()            \n",
    "        self.encoder = encoder \n",
    "        \n",
    "    def __len__(self):\n",
    "        return 900\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            b = self.train.next()\n",
    "        except StopIteration:\n",
    "            self.train.reset()\n",
    "            b = self.train.next()\n",
    "            \n",
    "        image = b.data[0].asnumpy()\n",
    "        bbox = b.label[0].asnumpy()[:, 0, 1:] * 256\n",
    "        label = b.label[0].asnumpy()[:, 0, 0]\n",
    "    \n",
    "        return torch.from_numpy(image), (torch.from_numpy(bbox), torch.from_numpy(label))   \n",
    "    \n",
    "    def collate_func(self, batch):\n",
    "\n",
    "        images = [b[0][0] for b in batch]\n",
    "        bbox = [b[1] for b in batch]\n",
    "        labels = [b[2] for b in batch]\n",
    "        \n",
    "        \n",
    "        encoded = [self.encoder.encode(bb, l, torch.Tensor([256, 256])) for bb, l in zip(bbox, labels)]\n",
    "        \n",
    "        \n",
    "        loc_target = [l[0] for l in encoded]\n",
    "        cls_target = [l[1] for l in encoded]\n",
    "        \n",
    "\n",
    "        return torch.stack(images) / 255, torch.stack(loc_target), torch.stack(cls_target)\n",
    "        \n",
    "        \n",
    "def down_sample(in_channels,out_channels):\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)       \n",
    "    )\n",
    "\n",
    "class SimpleSSD(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_cls=1, num_anchors=9):\n",
    "        super(SimpleSSD, self).__init__()\n",
    "        \n",
    "        self.n_cls = n_cls\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        \n",
    "        # Base CNN (think resnet/vgg or other base network)\n",
    "        self.step1 = down_sample(3, 128)\n",
    "        self.step2 = down_sample(128, 128)\n",
    "        self.step3 = down_sample(128, 128)\n",
    "        self.step4 = down_sample(128, 128)\n",
    "        self.step5 = down_sample(128, 128)\n",
    "\n",
    "\n",
    "        self.cls_head1 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head1 = nn.Conv2d(128, self.num_anchors *4, 3, padding=1)\n",
    "        \n",
    "        self.cls_head2 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head2 = nn.Conv2d(128,self.num_anchors * 4,3, padding=1)\n",
    "        \n",
    "        self.cls_head3 = nn.Conv2d(128, self.num_anchors * self.n_cls , 3, padding=1)\n",
    "        self.bbox_head3 = nn.Conv2d(128, self.num_anchors * 4,3, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        \n",
    "        step1=self.step1(x)\n",
    "        step2=self.step2(step1)\n",
    "\n",
    "        step3=self.step3(step2)\n",
    "        step4 = self.step4(step3)\n",
    "        step5 = self.step5(step4)\n",
    "                \n",
    "        cls1 = self.cls_head1(step3)\n",
    "        bbox1 = self.bbox_head1(step3)\n",
    "        \n",
    "        cls2 = self.cls_head2(step4)\n",
    "        bbox2 = self.bbox_head2(step4)\n",
    "        \n",
    "        cls3 = self.cls_head3(step5)\n",
    "        bbox3 = self.bbox_head3(step5) \n",
    "\n",
    "    \n",
    "        cls1 = cls1.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        cls2 = cls2.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        cls3 = cls3.permute(0,2,3,1).contiguous().view(x.size(0), -1, self.n_cls)\n",
    "        \n",
    "        bbox1 = bbox1.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox2 = bbox2.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "        bbox3 = bbox3.permute(0,2,3,1).contiguous().view(x.size(0), -1, 4)\n",
    "      \n",
    "        cls_pred = torch.cat([cls1, cls2, cls3], dim=1)\n",
    "        bbox_pred = torch.cat([bbox1, bbox2, bbox3], dim=1)\n",
    "        \n",
    "        return bbox_pred, cls_pred\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "pikachu_ds =PikachuDataset(anchor_areas=[30*30, 50*50, 90*90])\n",
    "pikachu_dl = DataLoader(pikachu_ds, batch_size=2, collate_fn=pikachu_ds.collate_func)\n",
    "\n",
    "# for b in pikachu_dl:\n",
    "#     break\n",
    "# import pandas as pd  \n",
    "# b[0].shape\n",
    "# b[1].shape\n",
    "# pd.DataFrame(b[2][0].cpu().numpy())[0].value_counts()\n",
    "# # b[0].shape\n",
    "\n",
    "# for b in pikachu_dl:\n",
    "#     break\n",
    "    \n",
    "# # b[0].shape\n",
    "# pikachu_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pikachu_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SimpleSSD(1)\n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(num_classes=1)\n",
    "# plot_losses = PlotLosses()\n",
    "\n",
    "\n",
    "\n",
    "net = NeuralNet(model, FocalLoss, criterion__num_classes=1)\n",
    "net.fit(pikachu_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "collect = []\n",
    "\n",
    "\n",
    "for epoch in range(2): \n",
    "    for i, b in enumerate(pikachu_dl):\n",
    "        logs = {}\n",
    "        optimizer.zero_grad()\n",
    "        image, bounding_boxes, labels = b\n",
    "        image = image.to(device)\n",
    "        bounding_boxes = bounding_boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        loc_pred, cls_pred = model(image)\n",
    "        total_loss = criterion(loc_pred, bounding_boxes, cls_pred, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        collect.append([total_loss.detach().cpu().numpy()])\n",
    "        logs['loss'] = total_loss.item()\n",
    "        if i % 10 == 0:\n",
    "            plot_losses.update(logs)\n",
    "            plot_losses.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "it = iter(pikachu_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = next(it)\n",
    "\n",
    "im = ToTensor()(Resize((256, 256))(ToPILImage()(torch.cat([b[0][0], b[0][0]], dim=1))))\n",
    "image = im.unsqueeze(0)\n",
    "\n",
    "enc = DataEncoder(anchor_areas=[30*30, 50*50, 90*90])\n",
    "loc_pred, cls_pred = model(image.cuda())\n",
    "\n",
    "i = 0\n",
    "bbspred, labelpred, score  = enc.decode(\n",
    "    loc_pred[i].float().cpu(), \n",
    "    cls_pred[i].float().cpu(), \n",
    "    torch.Tensor([256, 256]).float().cpu()\n",
    ")\n",
    "\n",
    "image_to_show = np.moveaxis(\n",
    "    image[i].detach().cpu().numpy(),0, 2)\n",
    "\n",
    "matched_anchors_on_image = ia.BoundingBoxesOnImage(\n",
    "    [ia.BoundingBox(*b) for b in bbspred.detach().cpu().numpy()], shape=(256, 256))\n",
    "\n",
    "image_to_show = matched_anchors_on_image.draw_on_image(image_to_show, thickness=2)\n",
    "plt.imshow(image_to_show)\n",
    "plt.title('score ' + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Load a custom image with pikachu( or many ) and try to make predictiosn with the network and visualize the result\n",
    "-  Can you think of anything that could confuse our detector? yellow dots ?\n",
    "- Currently the code is not really modular, try to make it nice by splitting it into logical parts\n",
    "    - Base feature extractor Module\n",
    "    - Head Creator module\n",
    "- Currently the detection/cls HEADS are very simple (just one CONV layer) they can be more complex. Try using more convolutions, check other architectures how its done\n",
    "\n",
    "- Can you use our network to train using some new data for instance:\n",
    "    - https://www.kaggle.com/tomluther/ships-in-google-earth\n",
    "    - https://www.kaggle.com/aruchomu/data-for-yolo-v3-kernel\n",
    "    - https://www.kaggle.com/dataturks/face-detection-in-images\n",
    "    - https://www.kaggle.com/dataturks/face-dataset-with-age-emotion-ethnicity  \n",
    " You will need to create a data loader/data sets similar as we did for the pikachu loader. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
